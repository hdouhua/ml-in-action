{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人工神经网络 VS 生物神经网络\n",
    "\n",
    "科学家们通过长久的探索，想让计算机像人一样思考，所以研发了人工神经网络，它究竟和我们的神经网络有多像?\n",
    "\n",
    "### 生物神经网络\n",
    "\n",
    "900亿神经细胞组成了我们复杂的神经网络系统，这个数量甚至可以和宇宙中的星球数相比较。如果仅仅靠单个的神经元，是永远没有办法让我们像今天一样，完成各种任务，处理各种复杂的问题。那我们是如何靠这些神经元来解决问题的呢? \n",
    "\n",
    "初始时期（婴儿），神经元并没有形成系统和网络，可能只是一些分散的细胞而已。当我们初次品尝糖果的时候，美妙的感觉，让我们的候神经元开始产生联结，记忆形成。我们的手和嘴产生了某种特定的搭配：每次发现有糖果的时候，某种生物信号就会从我们的嘴，通过之前形成的神经联结，传递到手上，让手的动作变得有意义！\n",
    "\n",
    "### 人工神经网络\n",
    "\n",
    "替代生物神经网络的是已经成体系的人工神经网络。所有神经元之间的连接都是固定不可更换的，在人工神经网络里，没有凭空产生新联结这回事。\n",
    "\n",
    "人工神经网络典型的一种学习方式就是：我们已经知道吃到糖果时手会如何动，但是我们需要让神经网络学着帮我们做这件动手的事情。\n",
    "\n",
    "我们预先准备好非常多吃糖的学习数据，然后将这些数据一次次放入这套人工神经网络系统中，糖的信号会通过这套系统传递到手，然后通过对比这次信号传递后手的动作是不是“讨糖”动作，来修改人工神经网络当中的*神经元强度*。\n",
    "\n",
    "这种修改在专业术语中叫做“**误差反向传递**”，也可以看作是再一次将传过来的信号传回去, 看看这个负责传递信号神经元对于”讨糖”的动作是否有贡献，让它好好反思与改正，争取下次做出更好的贡献。\n",
    "\n",
    "### 对比\n",
    "\n",
    "人工神经网络靠的是*正向*和*反向*传播来*更新神经元*，从而形成一个*好的神经系统*。本质上，这是一个**能让计算机处理和优化的数学模型**。\n",
    "\n",
    "生物神经网络是通过*刺激*，*产生新的联结*，让信号能够通过新的联结传递而*形成反馈*。我们身体里的神经系统经过了数千万年的进化，迄今为止，再庞大的人工神经网络系统也不能替代我们的小脑袋！\n",
    "\n",
    "人工神经网络和生物神经网络不是一回事。:)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why PyTorch\n",
    "\n",
    "PyTorch 是 Torch 在 Python 上的衍生。 Torch 很好用, 但是 Lua 不是特别流行，有开发团队将 Lua 的 Torch 移植到了更流行的语言 Python 上，就有了 PyTorch 。\n",
    "\n",
    "Torch 自称为神经网络界的 Numpy ，因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是环境里有合适的 GPU)，就像 Numpy 会把 array 放在 CPU 中加速运算。\n",
    "\n",
    "对比静态的 Tensorflow ，PyTorch 最大优点就是建立的神经网络是动态的，能更有效地处理一些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装\n",
    "\n",
    "参考 [官方文档](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "```bash\n",
    "# create a new env\n",
    "conda create --name neuro --clone base\n",
    "conda activate neuro\n",
    "# install pytorch\n",
    "conda install pytorch torchvision torchaudio -c pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 1.12.1\n",
      "tensor([[0.4479, 0.8858, 0.2628],\n",
      "        [0.4143, 0.8029, 0.4797],\n",
      "        [0.8151, 0.6282, 0.7688],\n",
      "        [0.5341, 0.9872, 0.9894],\n",
      "        [0.7362, 0.2418, 0.3836]])\n"
     ]
    }
   ],
   "source": [
    "# installation verification\n",
    "\n",
    "print(\"version:\", torch.__version__)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numpy array: [[0 1 2]\n",
      " [3 4 5]] \n",
      "torch tensor: tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) \n",
      "tensor to array: [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "np_data = np.arange(6).reshape((2, 3))\n",
    "\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "tensor2array = torch_data.numpy()\n",
    "\n",
    "print(\n",
    "    '\\nnumpy array:', np_data,          # [[0 1 2], [3 4 5]]\n",
    "    '\\ntorch tensor:', torch_data,      #  0  1  2 \\n 3  4  5    [torch.LongTensor of size 2x3]\n",
    "    '\\ntensor to array:', tensor2array, # [[0 1 2], [3 4 5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "abs \n",
      "numpy:  [1 2 1 2] \n",
      "torch:  tensor([1., 2., 1., 2.])\n",
      "\n",
      "sin \n",
      "numpy:  [-0.84147098 -0.90929743  0.84147098  0.90929743] \n",
      "torch:  tensor([-0.8415, -0.9093,  0.8415,  0.9093])\n",
      "\n",
      "mean \n",
      "numpy:  0.0 \n",
      "torch:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "data = [-1, -2, 1, 2]\n",
    "tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor\n",
    "\n",
    "print(\n",
    "    '\\nabs',\n",
    "    '\\nnumpy: ', np.abs(data),\n",
    "    '\\ntorch: ', torch.abs(tensor)\n",
    ")\n",
    "\n",
    "# sin   三角函数 sin\n",
    "print(\n",
    "    '\\nsin',\n",
    "    '\\nnumpy: ', np.sin(data),\n",
    "    '\\ntorch: ', torch.sin(tensor)\n",
    ")\n",
    "\n",
    "# mean  均值\n",
    "print(\n",
    "    '\\nmean',\n",
    "    '\\nnumpy: ', np.mean(data),\n",
    "    '\\ntorch: ', torch.mean(tensor)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matrix multiplication (matmul) \n",
      "numpy:  [[ 7 10]\n",
      " [15 22]] \n",
      "torch:  tensor([[ 7., 10.],\n",
      "        [15., 22.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication 矩阵点乘\n",
    "\n",
    "data = [[1,2], [3,4]]\n",
    "tensor = torch.FloatTensor(data)\n",
    "\n",
    "# correct method\n",
    "print(\n",
    "    '\\nmatrix multiplication (matmul)',\n",
    "    '\\nnumpy: ', np.matmul(data, data),     # [[7, 10], [15, 22]]\n",
    "    '\\ntorch: ', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]\n",
    ")\n",
    "\n",
    "# # !!!!  下面是错误的方法 !!!!\n",
    "# data = np.array(data)\n",
    "# print(\n",
    "#     '\\nmatrix multiplication (dot)',\n",
    "#     '\\nnumpy: ', data.dot(data),        # [[7, 10], [15, 22]] 在numpy 中可行\n",
    "#     '\\ntorch: ', tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0\n",
    "\n",
    "# 变为\n",
    "torch.dot(tensor.dot(tensor))\n",
    "# RuntimeError: 1D tensors expected, but got 2D and 2D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(torch.tensor([1,2,3,4]),torch.tensor([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 25.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tensor*tensor).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 10.],\n",
       "        [15., 22.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.FloatTensor([[1,2],[3,4]])\n",
    "# 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度\n",
    "variable = Variable(tensor, requires_grad=True)\n",
    "\n",
    "print(tensor)\n",
    "print(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5000)\n",
      "tensor(7.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_out = torch.mean(tensor*tensor)\n",
    "\n",
    "# 这步将在计算图中添加的一个计算步骤, 计算误差反向传递的时候有他一份功劳\n",
    "v_out = torch.mean(variable*variable)\n",
    "\n",
    "print(t_out)\n",
    "print(v_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个步骤，看不出两种计算有什么区别。但事实上, Variable 计算时, 它在背景幕布后面一步步默默地搭建着一个庞大的系统, 叫做计算图 computational graph 。这个图是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# v_out.backward()\n",
    "\n",
    "print(variable.grad)    # 初始 Variable 的梯度"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "757db7ef77c83814adbeeb3ac793403c6c9d7e2b87df671eda65f632ba05d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

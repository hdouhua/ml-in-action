# 损失函数

一个深度学习项目包括了模型的设计、损失函数的设计、梯度更新的方法、模型的保存与加载、模型的训练过程等几个主要模块。

<img src="https://static001.geekbang.org/resource/image/d7/fc/d76e19dd8d8a5a1bfdb4f4b1a17078fc.jpg?wh=1896x910" width="50%" />

损失函数是一把衡量模型学习效果的尺子，**训练模型的过程，实际就是优化损失函数的过程**。在机器学习中，常常提到的梯度更新和反向传播等内容，都是涉及到损失函数的相关概念。

## 拟合

模型的学习过程，模型最开始的时候就是一张白纸，它什么都不知道。研发人员不断地给模型提供要学习的数据。模型拿到数据之后就要有一个非常重要的环节：把模型自己的判断结果和数据真实的情况做比较。如果偏差或者差异特别大，那么模型就要去纠正自己的判断，用某种方式去减少这种偏差，然后反复这个过程，直到最后模型能够对数据进行正确的判断。
衡量这种偏差的方式很重要，也是模型学习进步的关键所在。减少偏差的过程，被称为拟合过程。

拟合会有过拟合与欠拟合。

## 损失函数与代价函数

假设一个二维空间中，任意一个点对应的真实函数为 F(x)，通过模型的学习拟合出来的函数为 f(x)。 F(x) 和 f(x) 之间存在一个误差，定义为 L(x)，于是有：

`L(x)=(F(x)−f(x))^2`

这里 F(x) 和 f(x) 的差距的平方和，是为了保证两者的误差是一个正值，方便后续的计算。也可以做成绝对值的形式，实践中平方和要比绝对值更为方便。

评价拟合函数表现效果“好坏”的度量指标——`损失函数（loss fuction)`。根据公式可知，损失函数越小，拟合函数对于真实情况的拟合效果就越好。L(x) 就是一种损失函数。（机器学习中损失函数有很多种）

把集合所有的点对应的拟合误差做平均，就会得到如下公式：

`∑​(F(x)−f(x))^2 / N` 

这个函数叫做`代价函数（cost function）`，即在训练样本集合上，所有样本的拟合误差的平均值。代价函数也称作经验风险。

在实际的应用中，并不需要严格区分损失函数和代价函数。只需要知道，损失函数是单个样本点的误差，代价函数是所有样本点的误差。

## 常见损失函数

严格来说，损失函数的种类是无穷多的。作为初学者，推荐从一些常用的损失函数做开始学习。

### 0-1 损失函数

如果模型预测对了，损失函数的值就为 0，因为没有误差；如果模型预测错了，那么损失函数的值就为 1。这就是最简单的 0-1 损失函数。
但是，0-1 损失函数的使用频率是非常少的，因为模型训练中经常用到的梯度更新和反向传播都需要能够求导的损失函数，可是 0-1 损失函数的导数值是 0（常数的导数为 0）。

### 平方损失函数

损失函数的定义时，举例的 L(x)=(F(x)−f(x))^2，这个函数的正式名称叫做平方损失函数。平方损失函数是可求导的损失函数中最简单的一种，它直接度量了模型拟合结果和真实结果之间的距离。

>有时候，我们会在损失函数中加入一个 1/2 的系数，这是为了求导的时候能够跟平方项的系数约掉。

### 均方差损失函数和平均绝对误差损失函数

`均方误差（Mean Squared Error，MSE）`是回归问题损失函数中最常用的一个，也称作 L2 损失函数。它是预测值与目标值之间差值的平方和。它的定义如下：

MSE=∑​(si​−yip​)^2 / n

其中 s 为目标值的向量表示，y 为预测值的向量表示。​

`平均绝对误差损失函数（Mean Absolute Error, MAE）`是另一种常用于回归问题的损失函数，它的目标是度量真实值和预测值差异的绝对值之和，定义如下：

MAE=∑|yi​−yip|​ / n

### 交叉熵损失函数

熵最开始是物理学中的一个术语，它表示了一个系统的混乱程度或者说无序程度。如果一个系统越混乱，那么它的熵越大。

后来，信息论创始人香农把这个概念引申到信道通信的过程中，开创了信息论，所以这里的熵又称为信息熵。信息熵的公式化可以表示为：

`H§=−∑​p(xi​)logp(xi​)`

其中，x 表示随机变量，与之相对应的是所有可能输出的集合。P(x) 表示输出概率函数。变量的不确定性越大，熵也就越大，把变量搞清楚所需要的信息量也就越大。

将函数变为如下格式，将 log p 改为 log q，即：

`−∑​p(xi​)log(q(xi​))`

其中，𝑝(𝑥) 表示真实概率分布，𝑞(𝑥) 表示预测概率分布。这个函数就是交叉熵损失函数（Cross entropy loss）。这个公式同时衡量了真实概率分布和预测概率分布两方面。通过不断尝试缩小两个概率分布的误差，使预测的概率分布尽可能达到真实概率分布。

### softmax 损失函数

softmax 是深度学习中使用非常频繁的一个函数。在某些场景下，一些数值大小范围分布非常广，而为了方便计算，或者使梯度更好的更新，需要把输入的这些数值映射为 0-1 之间的实数，并且归一化后能够保证几个数的和为 1。它的公式化表示为：

<img src="../res/images/torch-softmax.jpg" />

如果把交叉熵损失函数公式中的 q(xi)，也就是预测的概率分布，换成 softmax 方式的表示，即：

<img src="../res/images/torch-softmax-cross-entropy-loss.jpg" />

就得到了一个成为 softmax 损失函数（softmax loss）的新函数，也称为 softmax with cross-entropy loss，它是交叉熵损失函数的一个特例。

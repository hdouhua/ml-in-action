{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "Tensor 是深度学习框架中极为基础的概念，也是 PyTroch、TensorFlow 中最重要的知识点之一，它是一种数据的存储和处理结构。\n",
    "\n",
    "几种数据类型：\n",
    "\n",
    "- 标量，也称 Scalar，是一个只有大小，没有方向的量，比如 1.8、e、10 等。\n",
    "- 向量，也称 Vector，是一个有大小也有方向的量，比如 (1,2,3,4) 等。\n",
    "- 矩阵，也称 Matrix，是多个向量合并在一起得到的量，比如 [(1,2,3),(4,5,6)] 等。\n",
    "- 张量，也称 Tensor\n",
    "\n",
    "从标量、向量和矩阵的关系来看，它们就是不同“**维度**”的 Tensor ？\n",
    "\n",
    "更准确地使用 Rank（秩）来表示这种“维度”，比如标量，就是 Rank 为 0 阶的 Tensor；向量就是 Rank 为 1 阶的 Tensor；矩阵就是 Rank 为 2 阶的 Tensor；还有 Rank 大于 2 的 Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据类型\n",
    "\n",
    "一些常用类型\n",
    "\n",
    "<img src=\"https://static001.geekbang.org/resource/image/e6/08/e6af6a3b2172ee08db8c564146ae2108.jpg?wh=1680x933\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建\n",
    "\n",
    "**直接创建**\n",
    "\n",
    "```python\n",
    "torch.tensor(data, dtype=None, device=None,requires_grad=False)\n",
    "```\n",
    "\n",
    "- data，是要传入模型的数据。PyTorch 支持通过 list、 tuple、numpy array、scalar 等多种类型进行数据传入，并转换为 tensor；\n",
    "- dtype，它声明了需要返回的 Tensor 的类型；\n",
    "- device，这个参数指定了数据要返回到的设备；\n",
    "- requires_grad，用于说明当前量是否需要在计算中保留对应的梯度信息。\n",
    "\n",
    "  > requires_grad 的设置要灵活处理：\n",
    "  >- 如果是训练过程就要设置为 true，目的是方便求导、更新参数。而到了验证或者测试过程；\n",
    "  >- 如果目的是检查当前模型的泛化能力，就要设置成 Fasle，避免这个参数根据 loss 自动更新。\n",
    "\n",
    "**从 Numpy 创建**\n",
    "\n",
    "```python\n",
    "torch.from_numpy(ndarry)\n",
    "```\n",
    "\n",
    "**创建特殊形式的 Tensor**\n",
    "\n",
    "```python\n",
    "torch.zeros(*size, dtype=None...)\n",
    "torch.ones(size, dtype=None...)\n",
    "```\n",
    "\n",
    "**创建单位矩阵 Tensor**（单位矩阵是指主对角线上的元素都为 1 的矩阵）\n",
    "\n",
    "```python\n",
    "torch.eye(size, dtype=None...)\n",
    "```\n",
    "\n",
    "**创建随机矩阵 Tensor** 有以下几种较常使用的方式：\n",
    "\n",
    "```python\n",
    "torch.rand(size)\n",
    "torch.randn(size)\n",
    "torch.normal(mean, std, size)\n",
    "torch.randint(low, high, size)\n",
    "```\n",
    "\n",
    "- torch.rand 用于生成数据类型为浮点型且维度指定的随机 Tensor，随机生成的浮点数据在 0~1 区间均匀分布。\n",
    "- torch.randn 用于生成数据类型为浮点型且维度指定的随机 Tensor，随机生成的浮点数的取值满足均值为 0、方差为 1 的标准正态分布。\n",
    "- torch.normal 用于生成数据类型为浮点型且维度指定的随机 Tensor，可以指定均值和标准差。\n",
    "- torch.randint 用于生成随机整数的 Tensor，其内部填充的是在[low,high) 均匀生成的随机整数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 的转换\n",
    "\n",
    "**Int 与 Tensor 的转换**\n",
    "\n",
    "通过 torch.tensor 将一个数字（或者标量）转换为 Tensor，又通过 item() 函数，将 Tensor 转换为数字（标量），item() 函数的作用就是将 Tensor 转换为一个 python number。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(1)\n",
    "b = a.item()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**list 与 Tensor 的转换**\n",
    "\n",
    "使用 torch.tensor 就可以将其转换为 Tensor 了。而还原回来的过程要多一步，需要先将 Tensor 转为 NumPy 结构，之后再使用 tolist() 函数得到 list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = torch.tensor(a)\n",
    "c = b.numpy().tolist()\n",
    "\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NumPy 与 Tensor 的转换**\n",
    "\n",
    "使用 torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "numpy array:\n",
      " [[0 1 2]\n",
      " [3 4 5]] \n",
      "torch tensor:\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) \n",
      "tensor to array:\n",
      " [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "np_data = np.arange(6).reshape((2, 3))\n",
    "\n",
    "torch_data = torch.tensor(np_data)\n",
    "# torch_data = torch.from_numpy(np_data)\n",
    "\n",
    "tensor2array = torch_data.numpy()\n",
    "\n",
    "print(\n",
    "    \"\\nnumpy array:\\n\", np_data,\n",
    "    \"\\ntorch tensor:\\n\", torch_data,\n",
    "    \"\\ntensor to array:\\n\", tensor2array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor(np_data)\n",
    "torch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPU 与 GPU 的 Tensor 之间的转换**\n",
    "\n",
    "```\n",
    "CPU->GPU: data.cuda()\n",
    "GPU->CPU: data.cpu()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 的常用操作\n",
    "\n",
    "**获取形状**\n",
    "\n",
    "使用 shape 或 size 来获取。两者的不同之处在于，shape 是 Tensor对象 的一个属性，而 size() 则是一个 Tensor对象 拥有的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3, 5)\n",
    "print(a.shape)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**获取元素数量**\n",
    "\n",
    "知道了 Tensor 的形状，就很容易得到 Tensor 所包含的元素的数量了。具体的计算方法就是直接将所有维度的大小相乘，比如上面的 Tensor a 所含有的元素的个数为 2 * 3 * 5=30 个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**矩阵转秩 (维度转换）**\n",
    "\n",
    "在 PyTorch 中有两个函数： permute() 和 transpose() 可以用来实现矩阵的转秩，或者说交换不同维度的数据。\n",
    "比如在调整卷积层的尺寸、修改 channel 的顺序、变换全连接层的大小的时候，就要用到它们。\n",
    "\n",
    "- 函数 permute ，可以对任意高维矩阵进行转置，但只有 tensor.permute() 这个调用方式。\n",
    "- 函数 transpose，不同于 permute，它每次只能转换两个维度，或者说交换两个维度的数据。\n",
    "\n",
    "<img src=\"https://static001.geekbang.org/resource/image/02/84/025985c8635f3896d45d15e1ea381c84.jpg\" width=\"35%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "print(x.shape)\n",
    "\n",
    "# 2 表示原来第二个维度现在放在了第零个维度；\n",
    "# 1 表示原来第一个维度仍旧在第一个维度；\n",
    "# 0 表示原来第 0 个维度放在了现在的第 2 个维度。\n",
    "# 最后，形状就变成 [5,3,2]\n",
    "x = x.permute(2, 1, 0)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "print(x.shape)\n",
    "x = x.transpose(1, 0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.transpose(0, 2)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过了 transpose 或者 permute 处理之后的数据，变得不再连续了。内存虽然没有变化，但是的数据第 0 和第 2 维的数据发生了交换，现在的第 0 维是原来的第 2 维，所以 Tensor 都会变得不再连续。\n",
    "\n",
    "不连续，有啥影响？接着看。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**形状变换**\n",
    "\n",
    "在 PyTorch 中有两种常用的改变形状的函数，分别是 view 和 reshape。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([2, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6063, -1.3673, -0.1813, -0.1494,  0.4834, -1.7225,  0.0171,  1.8201],\n",
       "        [ 0.3441,  1.5952, -0.0528, -0.0723,  0.6460, -0.0282,  2.3335, -0.9216]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x.shape)\n",
    "x = x.view(2, 8)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "x = x.permute(1, 0)\n",
    "print(x.shape)\n",
    "\n",
    "# x.view(4, 4)\n",
    "\n",
    "# RuntimeError: view size is not compatible with input tensor's size and stride\n",
    "# (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抛出错误是由于经过 permute 后，将第 0 和第 1 维度的数据进行了变换，Tensor 变得不再连续了，而 view 无法处理不连续的 Tensor 。\n",
    "\n",
    "而 reshape 可以很好地工作，因为 reshape 相当于进行了两步操作，先把 Tensor 在内存中捋顺了，然后再进行 view 操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6063,  0.3441, -1.3673,  1.5952],\n",
       "        [-0.1813, -0.0528, -0.1494, -0.0723],\n",
       "        [ 0.4834,  0.6460, -1.7225, -0.0282],\n",
       "        [ 0.0171,  2.3335,  1.8201, -0.9216]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(4, 4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增减维度**\n",
    "\n",
    "有时候我们需要对 Tensor 增加或者删除某些维度，比如删除或者增加图片的几个通道。PyTorch 提供了 squeeze() 和 unsqueeze() 函数解决这个问题。\n",
    "\n",
    "- squeeze()：如果 dim 指定的维度的值为 1，则将该维度删除，若指定的维度值不为 1，则返回原来的 Tensor。\n",
    "- unsqueeze()：这个函数主要是对数据维度进行扩充。给指定位置加上维数为 1 的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 1, 3)\n",
    "print(x.shape)\n",
    "\n",
    "y = x.squeeze(1)\n",
    "print(y.shape)\n",
    "\n",
    "# 这里失败了：因为 y 此刻的第 1 维度的大小为 3，suqeeze 不能删除。\n",
    "z = y.squeeze(1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,1,3)\n",
    "y = x.unsqueeze(2)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "把 NumPy 和 Tensor 做对比，它们之间有很多共通的内容，两者都是数据的表示形式，都可以看作是科学计算的通用工具。但是 NumPy 和 Tensor 的用途是不一样的，NumPy 不能用于 GPU 加速，Tensor 则可以。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 的连接\n",
    "\n",
    "在项目开发中，深度学习某一层神经元的数据可能有多个不同的来源，那么就需要将数据进行组合，这个组合的操作，称之为连接。\n",
    "\n",
    "**cat**\n",
    "\n",
    "cat 连接操作函数，全称 concatnate。\n",
    "\n",
    "```python\n",
    "torch.cat(tensors, dim = 0, out = None)\n",
    "```\n",
    "- 第一个参数是 tensors，它很好理解，就是若干个准备进行拼接的 Tensor。\n",
    "- 第二个参数是 dim，是指拼接的维度（秩）。\n",
    "\n",
    "下面以二维 Tensor 来体验一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[1., 1., 1., 2., 2., 2.],\n",
      "        [1., 1., 1., 2., 2., 2.],\n",
      "        [1., 1., 1., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.ones(3, 3)\n",
    "B = 2 * torch.ones(3, 3)\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "# 拼接 dim=0\n",
    "# 按照“行”的方向拼接\n",
    "C = torch.cat([A, B], 0)\n",
    "print(C)\n",
    "\n",
    "# dim=1\n",
    "# 按照“列”的方向拼接的\n",
    "D = torch.cat([A, B], 1)\n",
    "print(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论 Tensor 是三维甚至更高维，cat 都会按照 dim 的数值维度方向链接两个 Tensor。\n",
    "\n",
    "cat 是将多个 Tensor 在已有的维度上进行连接，如果想增加新的维度进行连接，就需要 stack 函数登场了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stack**\n",
    "\n",
    "假设有两个二维矩阵 Tensor，把它们“堆叠”放在一起，构成一个三维的 Tensor 。这相当于原来的维度（秩）是 2，现在变成了 3，变成了一个立体的结构，增加了一个维度。\n",
    "\n",
    "```python\n",
    "torch.stack(inputs, dim=0)\n",
    "```\n",
    "\n",
    "inputs 表示需要拼接的 Tensor，dim 表示新建立维度的方向。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([5, 6, 7, 8])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [5, 6, 7, 8]])\n",
      "tensor([[0, 5],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [3, 8]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(0, 4)\n",
    "print(A)\n",
    "B = torch.arange(5, 9)\n",
    "print(B)\n",
    "\n",
    "# 在 dim=0，也就是“行”的方向上新建一个维度\n",
    "C = torch.stack((A, B), 0)\n",
    "print(C)\n",
    "\n",
    "# 在 dim=1，也就是“列”的方向上新建维度\n",
    "D = torch.stack((A, B), 1)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 的切分\n",
    "\n",
    "切分就是连接的逆过程，切分的操作主要分为三种类型：chunk、split、unbind。\n",
    "\n",
    "**chunk**\n",
    "\n",
    "chunk 的作用就是将 Tensor 按照声明的 dim，进行尽可能平均的划分。\n",
    "\n",
    "```python\n",
    "torch.chunk(input, chunks, dim=0)\n",
    "```\n",
    "\n",
    "- input，它表示要做 chunk 操作的 Tensor。\n",
    "- chunks，它代表将要被划分的块的数量，而不是每组的数量。请注意，chunks 必须是整型。\n",
    "- dim，就是按照哪个维度来进行 chunk。\n",
    "\n",
    ">chunk 参数不能够整除，chunk 函数是先做除法，然后再**向上取整**得到每组的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "(tensor([1, 2, 3, 4, 5]), tensor([ 6,  7,  8,  9, 10]))\n",
      "(tensor([1, 2, 3, 4]), tensor([5, 6, 7, 8]), tensor([ 9, 10]))\n",
      "torch.Size([4])\n",
      "torch.Size([4])\n",
      "torch.Size([2])\n",
      "(tensor([1, 2, 3, 4, 5]), tensor([ 6,  7,  8,  9, 10]), tensor([11, 12, 13, 14, 15]), tensor([16, 17]))\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 11)\n",
    "B = torch.chunk(A, 2, 0)\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "# math.ceil(10 / 3)\n",
    "C = torch.chunk(A, 3, 0)\n",
    "print(C)\n",
    "for t in C:\n",
    "    print(t.shape)\n",
    "\n",
    "# 18 / 4\n",
    "A1 = torch.arange(1, 18)\n",
    "C1 = torch.chunk(A1, 4, 0)\n",
    "print(C1)\n",
    "for t in C1:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "(tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]]), tensor([[ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]]))\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1,  2],\n",
       "         [ 5,  6],\n",
       "         [ 9, 10],\n",
       "         [13, 14]]),\n",
       " tensor([[ 3,  4],\n",
       "         [ 7,  8],\n",
       "         [11, 12],\n",
       "         [15, 16]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=torch.arange(1,17).reshape((4,4))\n",
    "print(A)\n",
    "\n",
    "B = torch.chunk(A, 2, 0)\n",
    "print(B)\n",
    "for t in B:\n",
    "    print(t.shape)\n",
    "\n",
    "C = torch.chunk(A, 2, 1)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split**\n",
    "\n",
    "按照“每份按照确定的大小”来进行切分\n",
    "\n",
    "```python\n",
    "torch.split(tensor, split_size_or_sections, dim=0)\n",
    "```\n",
    "\n",
    "- tensor，也就是待切分的 Tensor。\n",
    "- split_size_or_sections ，\n",
    "  - 当它为整数时，表示将 tensor 按照每块大小为这个整数的数值来切割；\n",
    "  - 当这个参数为列表时，则表示将此 tensor 切成和列表中元素一样大小的块。\n",
    "- dim，它定义了要按哪个维度切分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16],\n",
      "        [17, 18, 19, 20]])\n",
      "(tensor([[ 1,  2],\n",
      "        [ 5,  6],\n",
      "        [ 9, 10],\n",
      "        [13, 14],\n",
      "        [17, 18]]), tensor([[ 3,  4],\n",
      "        [ 7,  8],\n",
      "        [11, 12],\n",
      "        [15, 16],\n",
      "        [19, 20]]))\n",
      "(tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]]), tensor([[13, 14, 15, 16],\n",
      "        [17, 18, 19, 20]]))\n",
      "(tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]]), tensor([[ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16],\n",
      "        [17, 18, 19, 20]]))\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(1, 21).reshape(5, 4)\n",
    "print(A)\n",
    "\n",
    "# 按照每组2”列“的大小进行切分，得到了两个 5x2 大小的 Tensor\n",
    "C = torch.split(A, 2, 1)\n",
    "print(C)\n",
    "\n",
    "# 如果 split_size_or_sections 不能整除对应方向的大小\n",
    "# 得到一个 3x4 和一个 2x4 大小的 Tensor\n",
    "D = torch.split(A, 3, 0)\n",
    "print(D)\n",
    "\n",
    "# split_size_or_sections 是列表时的情况\n",
    "# 将此 tensor 切成和列表中元素大小一样的大小的块\n",
    "E = torch.split(A, (2, 3), 0)\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unbind**\n",
    "\n",
    "unbind 是一种降维切分的方式，相当于删除一个维度之后的结果。\n",
    "\n",
    "```python\n",
    "torch.unbind(input, dim=0)\n",
    "```\n",
    "\n",
    "- input 表示待处理的 Tensor，\n",
    "- dim 表示切片的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "(tensor([0, 1, 2, 3]), tensor([4, 5, 6, 7]), tensor([ 8,  9, 10, 11]), tensor([12, 13, 14, 15]))\n",
      "(tensor([ 0,  4,  8, 12]), tensor([ 1,  5,  9, 13]), tensor([ 2,  6, 10, 14]), tensor([ 3,  7, 11, 15]))\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(0, 16).view(4, 4)\n",
    "print(A)\n",
    "B = torch.unbind(A, 0)\n",
    "C = torch.unbind(A, 1)\n",
    "\n",
    "print(B)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引操作\n",
    "\n",
    "上面的操作都是对数据整体进行切分，并获得全部结果。有时候，我们只需要其中的一部分，要怎么做呢？直接告诉 Tensor 想要哪些部分，这种方法被称为索引操作。\n",
    "\n",
    "最常用的两个操作就是 index_select 和 masked_select 。\n",
    "\n",
    "**index_select**\n",
    "\n",
    "基于给定的索引来进行数据提取。\n",
    "\n",
    "```python\n",
    "torch.index_select(tensor, dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "tensor([[ 4,  5,  6,  7],\n",
      "        [12, 13, 14, 15]])\n",
      "tensor([[ 0,  3],\n",
      "        [ 4,  7],\n",
      "        [ 8, 11],\n",
      "        [12, 15]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(0, 16).view(4, 4)\n",
    "print(A)\n",
    "\n",
    "# 第 0 维选择第 1（行）和 3（行）的数据\n",
    "B = torch.index_select(A, 0, torch.tensor([1, 3]))\n",
    "print(B)\n",
    "\n",
    "# 选择第 0（列）和 3（列）的数据\n",
    "C = torch.index_select(A, 1, torch.tensor([0, 3]))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**masked_select**\n",
    "\n",
    "通过一些判断条件来进行数据提取。\n",
    "\n",
    "```python\n",
    "torch.masked_select(input, mask, out=None)\n",
    "```\n",
    "\n",
    "- input 表示待处理的 Tensor。\n",
    "- mask 代表掩码张量，也就是满足条件的特征掩码。需要注意：mask 须跟 input 张量的最高Rank/维的元素数相等，但形状或维度不需要相同。\n",
    "  比如，Rank = 2，那么就是列数目相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5941, 0.9153, 0.9745, 0.9504, 0.2550])\n",
      "tensor([ True,  True,  True,  True, False])\n",
      "tensor([0.5941, 0.9153, 0.9745, 0.9504])\n",
      "tensor([0.5941, 0.9153, 0.9745, 0.9504])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(5)\n",
    "print(A)\n",
    "\n",
    "B = A > 0.3\n",
    "print(B)\n",
    "\n",
    "# A 中 “满足 B 里面元素值为 True 的” 对应位置的数据\n",
    "C = torch.masked_select(A, B)\n",
    "print(C)\n",
    "\n",
    "# 上面可以简化为\n",
    "D = torch.masked_select(A, A > 0.3)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20]])\n",
      "torch.Size([4, 5])\n",
      "tensor([ True, False, False,  True, False])\n",
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  6,  9, 11, 14, 16, 19])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 21).reshape(4, 5)\n",
    "print(A)\n",
    "print(A.shape)\n",
    "\n",
    "# mask 的最高维元素数目 == A 的高维元素数目 5\n",
    "mask = torch.tensor([True, False, False, True, False])\n",
    "print(mask)\n",
    "print(mask.shape)\n",
    "\n",
    "torch.masked_select(A, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "Tensor 中的主要函数跟用法\n",
    "\n",
    "<img src=\"https://static001.geekbang.org/resource/image/d1/ba/d195706087f784c8e1e1c7c7b25a22ba.jpg?wh=3020x2455\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思考题\n",
    "\n",
    "有这样一个 tensor，要提取出其中第一行的第一个，第二行的第一、第二个，第三行的最后一个，该怎么做呢？\n",
    "\n",
    "```python\n",
    "A=torch.tensor([[4,5,7], [3,9,8],[2,3,4]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 9, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[4, 5, 7], [3, 9, 8], [2, 3, 4]])\n",
    "torch.masked_select(A, torch.tensor([[True, False, False],[True, True, False], [False, False, True]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "757db7ef77c83814adbeeb3ac793403c6c9d7e2b87df671eda65f632ba05d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

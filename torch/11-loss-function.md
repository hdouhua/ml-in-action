# 损失函数

一个深度学习项目包括了模型的设计、损失函数的设计、梯度更新的方法、模型的保存与加载、模型的训练过程等几个主要模块。

<img src="https://static001.geekbang.org/resource/image/d7/fc/d76e19dd8d8a5a1bfdb4f4b1a17078fc.jpg?wh=1896x910" width="50%" />

损失函数是一把衡量模型学习效果的尺子，**训练模型的过程，实际就是优化损失函数的过程**。在机器学习中，常常提到的梯度更新和反向传播等内容，都是涉及到损失函数的相关概念。

## 拟合

模型的学习过程，模型最开始的时候就是一张白纸，它什么都不知道。研发人员不断地给模型提供要学习的数据。模型拿到数据之后就要有一个非常重要的环节：把模型自己的判断结果和数据真实的情况做比较。如果偏差或者差异特别大，那么模型就要去纠正自己的判断，用某种方式去减少这种偏差，然后反复这个过程，直到最后模型能够对数据进行正确的判断。
衡量这种偏差的方式很重要，也是模型学习进步的关键所在。减少偏差的过程，被称为拟合过程。

拟合会有过拟合与欠拟合。

## 损失函数与代价函数

假设一个二维空间中，任意一个点对应的真实函数为 F(x)，通过模型的学习拟合出来的函数为 f(x)。 F(x) 和 f(x) 之间存在一个误差，定义为 L(x)，于是有：

`L(x)=(F(x)−f(x))^2`

这里 F(x) 和 f(x) 的差距的平方和，是为了保证两者的误差是一个正值，方便后续的计算。也可以做成绝对值的形式，实践中平方和要比绝对值更为方便。

评价拟合函数表现效果“好坏”的度量指标——`损失函数（loss fuction)`。根据公式可知，损失函数越小，拟合函数对于真实情况的拟合效果就越好。L(x) 就是一种损失函数。（机器学习中损失函数有很多种）

把集合所有的点对应的拟合误差做平均，就会得到如下公式：

`∑​(F(x)−f(x))^2 / N` 

这个函数叫做`代价函数（cost function）`，即在训练样本集合上，所有样本的拟合误差的平均值。代价函数也称作经验风险。

在实际的应用中，并不需要严格区分损失函数和代价函数。只需要知道，损失函数是单个样本点的误差，代价函数是所有样本点的误差。

## 常见损失函数

严格来说，损失函数的种类是无穷多的。作为初学者，推荐从一些常用的损失函数做开始学习。

### 0-1 损失函数

如果模型预测对了，损失函数的值就为 0，因为没有误差；如果模型预测错了，那么损失函数的值就为 1。这就是最简单的 0-1 损失函数。
但是，0-1 损失函数的使用频率是非常少的，因为模型训练中经常用到的梯度更新和反向传播都需要能够求导的损失函数，可是 0-1 损失函数的导数值是 0（常数的导数为 0）。

### 平方损失函数

损失函数的定义时，举例的 L(x)=(F(x)−f(x))^2，这个函数的正式名称叫做平方损失函数。平方损失函数是可求导的损失函数中最简单的一种，它直接度量了模型拟合结果和真实结果之间的距离。

>有时候，我们会在损失函数中加入一个 1/2 的系数，这是为了求导的时候能够跟平方项的系数约掉。

### 均方差损失函数和平均绝对误差损失函数

`均方误差（Mean Squared Error，MSE）`是回归问题损失函数中最常用的一个，也称作 L2 损失函数。它是预测值与目标值之间差值的平方和。它的定义如下：

MSE=∑​(si​−yip​)^2 / n

其中 s 为目标值的向量表示，y 为预测值的向量表示。​

`平均绝对误差损失函数（Mean Absolute Error, MAE）`是另一种常用于回归问题的损失函数，它的目标是度量真实值和预测值差异的绝对值之和，定义如下：

MAE=∑|yi​−yip|​ / n

### 交叉熵损失函数

熵最开始是物理学中的一个术语，它表示了一个系统的混乱程度或者说无序程度。如果一个系统越混乱，那么它的熵越大。

后来，信息论创始人香农把这个概念引申到信道通信的过程中，开创了信息论，所以这里的熵又称为信息熵。信息熵的公式化可以表示为：

`H§=−∑​p(xi​)logp(xi​)`

其中，x 表示随机变量，与之相对应的是所有可能输出的集合。P(x) 表示输出概率函数。变量的不确定性越大，熵也就越大，把变量搞清楚所需要的信息量也就越大。

将函数变为如下格式，将 log p 改为 log q，即：

`−∑​p(xi​)log(q(xi​))`

其中，𝑝(𝑥) 表示真实概率分布，𝑞(𝑥) 表示预测概率分布。这个函数就是交叉熵损失函数（Cross entropy loss）。这个公式同时衡量了真实概率分布和预测概率分布两方面。通过不断尝试缩小两个概率分布的误差，使预测的概率分布尽可能达到真实概率分布。

### softmax 损失函数

softmax 是深度学习中使用非常频繁的一个函数。在某些场景下，一些数值大小范围分布非常广，而为了方便计算，或者使梯度更好的更新，需要把输入的这些数值映射为 0-1 之间的实数，并且归一化后能够保证几个数的和为 1。它的公式化表示为：

<img src="../res/images/torch-softmax.jpg" />

如果把交叉熵损失函数公式中的 q(xi)，也就是预测的概率分布，换成 softmax 方式的表示，即：

<img src="../res/images/torch-softmax-cross-entropy-loss.jpg" />

就得到了一个成为 softmax 损失函数（softmax loss）的新函数，也称为 softmax with cross-entropy loss，它是交叉熵损失函数的一个特例。

# 计算梯度

模型有了损失函数，才能够进行学习。那么问题来了，模型是如何通过损失函数进行学习的呢？

## 前馈网络

前馈网络，也称为前馈神经网络，是一种“往前走”的神经网络。它是最简单的神经网络，其典型特征是一个单向的多层结构。

<img src="https://static001.geekbang.org/resource/image/2c/27/2c89c49e8a4d46724113874c2e8d8f27.jpg?wh=1241x790" width="50%" />

最左侧的绿色的一个个神经元，它们相当于第 0 层，一般适用于接收输入数据的层 ——输入层。

比如，要训练一个 y=f(x) 函数的神经网络，x 作为一个向量，就需要通过这个绿色的输入层进入模型。在这个网络中，输入层有 5 个神经元，这意味着它可以接收一个 5 维长度的向量。

网络的中间有一层红色的神经元，它们相当于模型的“内部”，一般来说对外不可见，或者使用者并不关心的非结果部分 ——隐藏层。
在实际的网络模型中，隐藏层会有非常多的层数，它们是网络最为关键的内部核心，也是模型能够学习知识的关键部分。

在图的右侧，蓝色的神经元是网络的最后一层。模型内部计算完成之后，就需要通过这一层输出到外部 ——输出层。

神经元之间的连线，表示神经元之间连接的权重，通过权重就会知道网络中每个节点的重要程度。

在前馈网络中，数据从输入层进入到隐藏层的第一层，然后传播到第二层，第三层……一直到最后通过输出层输出。数据的传播是单向的，无法后退，只能前行。

## 导数、梯度与链式法则

有前向的数据传播，自然也会有反向的数据传播过程。
说到反向传播，常常还会把它与梯度下降、链式法则这些相关联。

### 导数

导数，也叫做导函数值。

以 F=3x 为例，在 x=3 的时候，函数值为 3x=3*3=9。现在给 x 一个非常小的增量Δx，那么就有了 F(x+Δx)=3(x+Δx)，也就是说函数值也有了一个非常小的增量，记为Δy。当函数值增量Δy 与变量 x 的增量Δx 的比值，在Δx 趋近于 0 时，如果极限 a 存在，就称 a 为函数 F(x) 在 x 处的导数。

### 偏导数

有的函数不止一个变量，比如 z=3x+2y 函数中就同时存在了 x 和 y 两种变量，那该怎么求它们的导数呢？这是偏导数的问题。

偏导数其实就是保持一个变量变化，而所有其他变量恒定不变的求导过程。

比如对于函数 z=x^2+y^2，∂x/∂z​=2x 表示函数 z 在 x 上的导数，∂y/∂z​=2y 表示函数 z 在 y 上的导数。

### 梯度

函数所有偏导数构成的向量就叫做梯度。

梯度向量的方向即为函数值增长最快的方向。——这是一个非常重要的结论，它贯穿了整个深度学习的全过程。模型要学习知识，就要用最快最好的方式来完成，其实就是需要借助梯度来进行。

### 链式法则

**深度学习的整个学习过程，其实就是一个更新网络节点之前权重的过程**。这个权重就在前馈网络中示意图中看到的节点之间的连线，一般使用 w 来进行表示它。

模型就是通过*不断地减小损失函数值*的方式来进行学习的。让损失函数最小化，通常就要采用*梯度下降*的方式，即：每一次给模型的权重进行更新的时候，都要按照梯度的反方向（梯度向量的方向即为函数值增长最快的方向，反方向则是减小最快的方向）进行。

**链式法则：“两个函数组合起来的复合函数，导数等于里面函数代入外函数值的导数，乘以里面函数之导数。”**

<img src="../res/images/torch-chain-rule.jpg" width="50%" />

有函数 f(x)=cos(x2−1)，可以把函数分解为：
1. f(x)=cos(x)
2. g(x)=x^2−1

g(x) 的导数 g′(x)=2x，f(x) 的导数 f′(x)=−sin(x)，则 `f′(x)=f′(g(x)) * g′(x)=−sin(x∧2−1) * 2x`，相当于各自求导后再相乘。

## 反向传播

反向传播算法（Backpropagation）是目前训练神经网络最常用且最有效的算法。模型就是通过反向传播的方式来不断更新自身的参数，从而实现了“学习”知识的过程。

反向传播的主要原理是：
- 前向传播：数据从输入层经过隐藏层最后输出，其过程和之前讲过的前馈网络基本一致。
- 计算误差并传播：计算模型输出结果和真实结果之间的误差，并将这种误差通过某种方式反向传播，即从输出层向隐藏层传递并最后到达输入层。
- 迭代：在反向传播的过程中，根据误差不断地调整模型的参数值，并不断地迭代前面两个步骤，直到达到模型结束训练的条件。

其中最重要的环节有两个：一是通过某种方式反向传播；二是根据误差不断地调整模型的参数值。——统称为优化方法

## 小结

模型通过梯度下降的方式，在梯度方向的反方向上不断减小损失函数值，从而进行学习。

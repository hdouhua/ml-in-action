# 入门 Spark

## Spark 的“快”和“全”

**快**，有两个方面，一个是开发效率快，另一个是执行效率快。Spark 支持多种开发语言，如 Python、Java、Scala、R 和 SQL，同时提供了种类丰富的开发算子，如 RDD、DataFrame、Dataset。这些特性让开发者能够像搭积木一样，信手拈来、驾轻就熟地完成数据应用开发。

许多从零开始用 Spark 做开发的同学。最开始，他们往往需要“照葫芦画瓢”、参考别人的代码实现才能完成自己的工作。但是，经过短短 3 个月的强化练习之后，绝大多数同学都能够独当一面、熟练地实现各式各样的业务需求。而这，自然要归功于 Spark 框架本身超高的开发效率。

凭借 Spark Core 和 Spark SQL 这两个并驾齐驱的计算引擎，开发出的数据应用并不需要太多的调整或是优化，就能享有不错的执行性能。

**全**，指的是 Spark 在计算场景的支持上非常全面。在数据应用领域，有如下几类计算场景，它们分别是批处理、流计算、数据分析、机器学习和图计算。

对于这几类计算场景，Spark 提供了丰富的子框架予以支持。比如，针对批处理的 Spark Core，流计算的 Structured Streaming，用于数据分析的 Spark SQL，服务于机器学习的 Spark MLlib，图计算 Spark GraphFrames 等等。

对于想要在数据应用领域有所建树的同学来说，Spark 可以说是一门必修课。

## 入门“三步走”：

第一步，需要掌握 Spark 常用的开发 API 与开发算子。通过这些 API 与开发算子，才能启动并驱使 Spark 的分布式计算引擎。

第二步，要想让 Spark 这台车子跑得稳，必须要深入理解它的工作原理。

第三步，需要了解并熟悉 Spark 不同的计算子框架（Spark SQL、Spark MLlib 和 Structured Streaming），来应对不同的数据应用场景，比如数据分析、机器学习和流计算。

## 四个模块

![4-modules](https://static001.geekbang.org/resource/image/61/bf/615da0cba86c806caf1afc26bcd10dbf.jpg?wh=2284x797)

## 知识体系

![course-mindmap](https://static001.geekbang.org/resource/image/eb/a2/eb5d573a0800c1457774479c6a91fda2.jpg?wh=2284x797)

## 参考 / 扩展

- [课程](https://time.geekbang.org/column/intro/100090001)
- [Spark By Examples | Learn Spark Tutorial with Examples](https://sparkbyexamples.com/)

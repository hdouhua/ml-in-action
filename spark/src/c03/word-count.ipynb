{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/07 22:03:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkContext = SparkContext()\n",
    "textFile = sparkContext.textFile(\"../../res/wikiOfSpark.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "def f(word: String): (String, Int) = {\n",
    "    return (word, 1)\n",
    "}\n",
    "\n",
    "val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换成 python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逐行分词\n",
    "cleanWordRDD= textFile.flatMap(lambda line: line.split(\" \")).filter(lambda word : word != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Apache', 1), ('Spark', 1), ('From', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(word):\n",
    "    return (word, 1)\n",
    "\n",
    "kvRDD = cleanWordRDD.map(f)\n",
    "\n",
    "kvRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val kvRDD: RDD[(String, Int)] = cleanWordRDD.map{ word =>\n",
    "    // 获取MD5对象实例\n",
    "    val md5 = MessageDigest.getInstance(\"MD5\")\n",
    "    // 使用MD5计算哈希值\n",
    "    val hash = md5.digest(word.getBytes).mkString\n",
    "    // 返回哈希值与数字1的Pair\n",
    "    (hash, 1)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9713ae04a02a810d6f33dd956f42794', 1),\n",
       " ('8cde774d6f7333752ed72cacddb05126', 1),\n",
       " ('5da618e8e4b89c66fe86e32cdafde142', 1),\n",
       " ('82b4ef4154f1823e73cf6191e307196c', 1),\n",
       " ('8fc42c6ddf9966db3b09e84365034357', 1),\n",
       " ('aa2d6e4f578eb0cfaba23beef76c2194', 1),\n",
       " ('f2340a1d30f79f3b1c1d78ca4fadbd26', 1),\n",
       " ('101f693f72287a2819a364f64ca1c0ed', 1),\n",
       " ('01b6e20344b68835c5ed1ddedf20d531', 1),\n",
       " ('600d34c7b047239b5fe0f3383f584d95', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(word):\n",
    "    hash = md5(word.encode(\"utf8\"))\n",
    "    return hash.hexdigest()\n",
    "\n",
    "kvRDD = (\n",
    "    cleanWordRDD\n",
    "    .map(f)\n",
    "    .map(lambda word: (word, 1))\n",
    ")\n",
    "\n",
    "kvRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 mapPartitions 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9713ae04a02a810d6f33dd956f42794', 1),\n",
       " ('8cde774d6f7333752ed72cacddb05126', 1),\n",
       " ('5da618e8e4b89c66fe86e32cdafde142', 1),\n",
       " ('82b4ef4154f1823e73cf6191e307196c', 1),\n",
       " ('8fc42c6ddf9966db3b09e84365034357', 1),\n",
       " ('aa2d6e4f578eb0cfaba23beef76c2194', 1),\n",
       " ('f2340a1d30f79f3b1c1d78ca4fadbd26', 1),\n",
       " ('101f693f72287a2819a364f64ca1c0ed', 1),\n",
       " ('01b6e20344b68835c5ed1ddedf20d531', 1),\n",
       " ('600d34c7b047239b5fe0f3383f584d95', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(partition):\n",
    "    for word in partition:\n",
    "        hash= md5(word.encode(\"utf8\"))\n",
    "        yield hash.hexdigest()\n",
    "\n",
    "kvRDD = (\n",
    "    cleanWordRDD\n",
    "    .mapPartitions(f)\n",
    "    .map(lambda word: (word, 1))\n",
    ")\n",
    "\n",
    "kvRDD.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```scala\n",
    "val wordPairRDD: RDD[String] = lineRDD.flatMap( line => {\n",
    "    // 将行转换为单词数组\n",
    "    val words: Array[String] = line.split(\" \")\n",
    "    // 将单个单词数组，转换为相邻单词数组\n",
    "    for (i <- 0 until words.length - 1) yield words(i) + \"-\" + words(i+1)\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache-Spark',\n",
       " 'From-Wikipedia,',\n",
       " 'Wikipedia,-the',\n",
       " 'the-free',\n",
       " 'free-encyclopedia']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(line):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(len(words) - 1):\n",
    "        yield words[i] + \"-\" + words[i+1]\n",
    "\n",
    "wordPairRDD = textFile.flatMap(f)\n",
    "\n",
    "wordPairRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "// 定义特殊字符列表\n",
    "val list: List[String] = List(\"&\", \"|\", \"#\", \"^\", \"@\")\n",
    " \n",
    "// 定义判定函数f\n",
    "def f(s: String): Boolean = {\n",
    "    val words: Array[String] = s.split(\"-\")\n",
    "    val b1: Boolean = list.contains(words(0))\n",
    "    val b2: Boolean = list.contains(words(1))\n",
    "    return !b1 && !b2 // 返回不在特殊字符列表中的词汇对\n",
    "}\n",
    " \n",
    "// 使用filter(f)对RDD进行过滤\n",
    "val cleanedPairRDD: RDD[String] = wordPairRDD.filter(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.3.0-|', '|-Apache', 'Cassandra-|', '|-Pluralsight\".', '\"MLlib-|']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars = [\"&\", \"|\", \"#\", \"^\", \"@\"]\n",
    "\n",
    "\n",
    "def f(str):\n",
    "    words = str.split(\"-\")\n",
    "    b1 = words[0] in special_chars\n",
    "    b2 = words[1] in special_chars\n",
    "\n",
    "    # return not (b1 or b2)\n",
    "    return b1 or b2\n",
    "\n",
    "\n",
    "cleanedPairRDD = wordPairRDD.filter(f)\n",
    "\n",
    "cleanedPairRDD.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5ca8aa162d6e2ef15460f43a51f4411e8aec4bf04fea983283c66e0530195d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/08 00:13:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sparkContext = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备\n",
    "\n",
    "lineRDD = sparkContext.textFile(\"../../res/wikiOfSpark.txt\")\n",
    "cleanWordRDD = (\n",
    "    lineRDD\n",
    "    .flatMap(lambda line: line.split(\" \"))\n",
    "    .filter(lambda word: word != \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map 以元素为粒度的数据转换\n",
    "\n",
    "<hr />\n",
    "\n",
    "```scala\n",
    "def f(word: String): (String, Int) = {\n",
    "    return (word, 1)\n",
    "}\n",
    "\n",
    "val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(f)\n",
    "```\n",
    "<hr />\n",
    "\n",
    "转换成 python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Apache', 1), ('Spark', 1), ('From', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(word):\n",
    "    return (word, 1)\n",
    "\n",
    "\n",
    "kvRDD = cleanWordRDD.map(f)\n",
    "\n",
    "kvRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapPartitions：以数据分区为粒度的数据转换\n",
    "\n",
    "map 实现方式\n",
    "\n",
    "<hr />\n",
    "\n",
    "```scala\n",
    "val kvRDD: RDD[(String, Int)] = cleanWordRDD.map{ word =>\n",
    "    // 获取MD5对象实例\n",
    "    val md5 = MessageDigest.getInstance(\"MD5\")\n",
    "    // 使用MD5计算哈希值\n",
    "    val hash = md5.digest(word.getBytes).mkString\n",
    "    // 返回哈希值与数字1的Pair\n",
    "    (hash, 1)\n",
    "}\n",
    "```\n",
    "<hr />\n",
    "\n",
    "转换成 python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9713ae04a02a810d6f33dd956f42794', 1),\n",
       " ('8cde774d6f7333752ed72cacddb05126', 1),\n",
       " ('5da618e8e4b89c66fe86e32cdafde142', 1),\n",
       " ('82b4ef4154f1823e73cf6191e307196c', 1),\n",
       " ('8fc42c6ddf9966db3b09e84365034357', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "\n",
    "def f(word):\n",
    "    hash = md5(word.encode(\"utf8\"))\n",
    "    return (hash.hexdigest(), 1)\n",
    "\n",
    "\n",
    "kvRDD = cleanWordRDD.map(f)\n",
    "\n",
    "kvRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 mapPartitions 优化\n",
    "\n",
    "<hr />\n",
    "\n",
    "```scala\n",
    "val kvRDD: RDD[(String, Int)] = cleanWordRDD.mapPartitions( partition => {\n",
    "    // 注意！这里是以数据分区为粒度，获取MD5对象实例\n",
    "    val md5 = MessageDigest.getInstance(\"MD5\")\n",
    "    val newPartition = partition.map( word => {\n",
    "    // 在处理每一条数据记录的时候，可以复用同一个Partition内的MD5对象\n",
    "    (md5.digest(word.getBytes()).mkString,1)\n",
    "    })\n",
    "    newPartition\n",
    "})\n",
    "```\n",
    "<hr />\n",
    "\n",
    "转 Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9713ae04a02a810d6f33dd956f42794', 1),\n",
       " ('8cde774d6f7333752ed72cacddb05126', 1),\n",
       " ('5da618e8e4b89c66fe86e32cdafde142', 1),\n",
       " ('82b4ef4154f1823e73cf6191e307196c', 1),\n",
       " ('8fc42c6ddf9966db3b09e84365034357', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(partition):\n",
    "    for word in partition:\n",
    "        hash = md5(word.encode(\"utf8\"))\n",
    "        yield (hash.hexdigest(), 1)\n",
    "\n",
    "\n",
    "kvRDD = cleanWordRDD.mapPartitions(f)\n",
    "\n",
    "kvRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e9713ae04a02a810d6f33dd956f42794', 1),\n",
       " ('8cde774d6f7333752ed72cacddb05126', 1),\n",
       " ('5da618e8e4b89c66fe86e32cdafde142', 1),\n",
       " ('82b4ef4154f1823e73cf6191e307196c', 1),\n",
       " ('8fc42c6ddf9966db3b09e84365034357', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(idx, partition):\n",
    "    for word in partition:\n",
    "        hash = md5(word.encode(\"utf8\"))\n",
    "        yield (hash.hexdigest(), 1)\n",
    "\n",
    "\n",
    "kvRDD = cleanWordRDD.mapPartitionsWithIndex(f)\n",
    "\n",
    "kvRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatMap：从元素到集合、再从集合到元素\n",
    "\n",
    "<hr />\n",
    "\n",
    "```scala\n",
    "val wordPairRDD: RDD[String] = lineRDD.flatMap( line => {\n",
    "    // 将行转换为单词数组\n",
    "    val words: Array[String] = line.split(\" \")\n",
    "    // 将单个单词数组，转换为相邻单词数组\n",
    "    for (i <- 0 until words.length - 1) yield words(i) + \"-\" + words(i+1)\n",
    "})\n",
    "```\n",
    "<hr />\n",
    "\n",
    "转换成 python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache-Spark',\n",
       " 'From-Wikipedia,',\n",
       " 'Wikipedia,-the',\n",
       " 'the-free',\n",
       " 'free-encyclopedia']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(line):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(len(words) - 1):\n",
    "        yield words[i] + \"-\" + words[i + 1]\n",
    "\n",
    "\n",
    "wordPairRDD = lineRDD.flatMap(f)\n",
    "\n",
    "wordPairRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter：过滤\n",
    "\n",
    "<hr />\n",
    "\n",
    "```scala\n",
    "// 定义特殊字符列表\n",
    "val list: List[String] = List(\"&\", \"|\", \"#\", \"^\", \"@\")\n",
    " \n",
    "// 定义判定函数f\n",
    "def f(s: String): Boolean = {\n",
    "    val words: Array[String] = s.split(\"-\")\n",
    "    val b1: Boolean = list.contains(words(0))\n",
    "    val b2: Boolean = list.contains(words(1))\n",
    "    return !b1 && !b2 // 返回不在特殊字符列表中的词汇对\n",
    "}\n",
    " \n",
    "// 使用filter(f)对RDD进行过滤\n",
    "val cleanedPairRDD: RDD[String] = wordPairRDD.filter(f)\n",
    "```\n",
    "<hr />\n",
    "\n",
    "转换成 python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.3.0-|', '|-Apache', 'Cassandra-|', '|-Pluralsight\".', '\"MLlib-|']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars = [\"&\", \"|\", \"#\", \"^\", \"@\"]\n",
    "\n",
    "\n",
    "def f(str):\n",
    "    words = str.split(\"-\")\n",
    "    b1 = words[0] in special_chars\n",
    "    b2 = words[1] in special_chars\n",
    "\n",
    "    # return not (b1 or b2)\n",
    "    return b1 or b2\n",
    "\n",
    "\n",
    "cleanedPairRDD = wordPairRDD.filter(f)\n",
    "\n",
    "cleanedPairRDD.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5ca8aa162d6e2ef15460f43a51f4411e8aec4bf04fea983283c66e0530195d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

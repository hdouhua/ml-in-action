# Shuffle 管理

任务调度的首要环节，是 DAGScheduler 以 Shuffle 为边界，把计算图 DAG 切割为多个执行阶段 Stages。

由于 Shuffle 的计算几乎需要消耗所有类型的硬件资源，比如 CPU、内存、磁盘与网络等，因此，在绝大多数的 Spark 作业中，Shuffle 是作业执行性能的瓶颈。
>具体来说，Shuffle 中的哈希与排序操作会大量消耗 CPU，而 Shuffle Write 生成中间文件的过程，会消耗宝贵的内存资源与磁盘 I/O，最后，Shuffle Read 阶段的数据拉取会引入大量的网络 I/O。=> Shuffle 是资源密集型计算。

——Shuffle 是衔接不同执行阶段的关键环节，Shuffle 的执行性能往往是 Spark 作业端到端执行效率的关键环节。

## Shuffle 是什么

Shuffle 的本意是扑克的“洗牌”，在分布式计算场景中，它被引申为**集群范围内跨节点、跨进程的数据分发**。

在工地搬砖的任务中，把不同类型的砖头看作是分布式数据集，不同类型的砖头在各个分公司之间搬运的过程，与分布式计算中的 Shuffle 可以说是异曲同工。

要完成工地搬砖的任务，每位工人都需要长途跋涉到另外两家分公司，然后从人家的临时仓库把所需的砖头搬运回来。分公司之间相隔甚远，仅靠工人们一块砖一块砖地搬运，显然不现实。因此，为了提升搬砖效率，每位工人还需要借助货运卡车来帮忙。不难发现，工地搬砖的任务需要消耗大量的人力物力，可以说是劳师动众。

在 Shuffle 的过程，分布式数据集在集群内的分发，会引入大量的磁盘 I/O 与网络 I/O。在 DAG 的计算链条中，Shuffle 环节的执行性能是最差的。

既然 Shuffle 的性能这么差，为什么在计算的过程中非要引入 Shuffle 操作呢？免去 Shuffle 环节不行吗？

计算过程之所以需要 Shuffle，往往是由计算逻辑、或者说业务逻辑决定的。结合过往的工作经验，在绝大多数的业务场景中，Shuffle 操作都是必需的、无法避免的。

比如，对于搬砖任务来说，不同的建筑项目就是需要不同的建材，只有这样才能满足不同的施工要求。

再比如，在 Word Count 的例子中，“业务逻辑”是对单词做统计计数，那么对单词“Spark”来说，在做“加和”之前，就得把原本分散在不同 Executors 中的“Spark”，拉取到某一个 Executor，才能完成统计计数的操作。

## Shuffle 工作原理

以 Word Count 的 reduceByKey 为例，

[参考代码](./src/c01/word-count.scala)

```
val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) => x + y) 
```

<img src="https://static001.geekbang.org/resource/image/31/4d/3199582354a56f9e64bdf7b8a516b04d.jpg?wh=1920x1256" width="50%" />

以 Shuffle 为边界，reduceByKey 的计算被切割为两个执行阶段。把 Shuffle 之前的 Stage 叫作 Map 阶段，而把 Shuffle 之后的 Stage 称作 Reduce 阶段。
- 在 Map 阶段，每个 Executors 先把自己负责的数据分区做初步聚合（又叫 Map 端聚合、局部聚合）；
- 在 Shuffle 环节，不同的单词被分发到不同节点的 Executors 中；
- 在 Reduce 阶段，Executors 以单词为 Key 做第二次聚合（又叫全局聚合），从而完成统计计数的任务。

以上结论：与其说 Shuffle 是跨节点、跨进程的数据分发，不如说 Shuffle 是 Map 阶段与 Reduce 阶段之间的数据交换。

### Shuffle 中间文件

Map 阶段与 Reduce 阶段，通过 Map 阶段 生产 Shuffle 中间文件，Reduce 阶段消费这个中间文件，以中间文件为媒介，来完成集群范围内的数据交换。

**中间文件的产生和消费过程**

<img src="https://static001.geekbang.org/resource/image/95/80/95479766b8acebdedd5c8a0f8bda0680.jpg?wh=1920x862" width="50%" />

解释：

调度系统里 DAGScheduler 会为每一个 Stage 创建任务集合 TaskSet，而每一个 TaskSet 都包含多个分布式任务（Task）。
- 在 Map 执行阶段，每个 Task（以下简称 Map Task）都会生成包含 data 文件与 index 文件的 Shuffle 中间文件，如上图所示。
   Shuffle 文件的生成，是以 Map Task 为粒度的，Map 阶段有多少个 Map Task，就会生成多少份 Shuffle 中间文件。
- Shuffle 中间文件是统称、泛指，它包含两类实体文件：*记录（Key，Value）键值对的 data 文件* 和 *记录键值对所属 Reduce Task 的 index 文件*。
   - index 文件标记了 data 文件中的哪些记录，应该由下游 Reduce 阶段中的哪些 Task（简称 Reduce Task）消费。
   - 在 Spark 中，*Shuffle 环节数据交换规则又叫分区规则*，它定义了分布式数据集在 Reduce 阶段如何划分数据分区。

     假设 Reduce 阶段有 N 个 Task，这 N 个 Task 对应着 N 个数据分区，那么在 Map 阶段，每条记录应该分发到 Reduce Task 的 P 号分区，公式如下，
     ```
     P = Hash(Record Key) % N
     ```

>对于每一个 Map Task 生成的中间文件，其中的目标分区数量是由 Reduce 阶段的任务数量（又叫并行度）决定的。  
>在上面的示意图中，Reduce 阶段的并行度是 3，因此，Map Task 的中间文件会包含 3 个目标分区的数据， index 文件是用来标记目标分区所属数据记录的起始索引。

### Shuffle Write

<img src="https://static001.geekbang.org/resource/image/92/ab/92781f6ff67224812d7aee1b7d6a63ab.jpg?wh=1920x618" width="70%" />

解释：

在生成中间文件的过程中，Spark 会借助一种类似于 Map 的数据结构，来计算、缓存并排序数据分区中的数据记录。这种 Map 结构的 Key 是（Reduce Task Partition ID，Record Key），而 Value 是原数据记录中的数据值，如图中的“内存数据结构”所示。

对于数据分区中的数据记录，Spark 会根据前面提到的公式逐条计算记录所属的目标分区 ID，然后把主键（Reduce Task Partition ID，Record Key）和记录的数据值插入到 Map 数据结构中。当 Map 结构被灌满之后，Spark 根据主键对 Map 中的数据记录做排序，然后把所有内容溢出到磁盘中的临时文件。

随着 Map 结构被清空，Spark 可以继续读取分区内容并继续向 Map 结构中插入数据，直到 Map 结构再次被灌满而再次溢出。就这样，如此往复，直到数据分区中所有的数据记录都被处理完毕。

到此，磁盘上存有若干个溢出的临时文件，而内存的 Map 结构中留有部分数据，Spark 使用归并排序算法对所有临时文件和 Map 结构剩余数据做合并，分别生成 data 文件、和与之对应的 index 文件。

*Shuffle 阶段生成中间文件的过程，又叫 Shuffle Write*。

Shuffle Write 过程中生成中间文件的详细过程，归纳起来，这个过程分为 4 个步骤：
1. 对于数据分区中的数据记录，逐一计算其目标分区，然后填充内存数据结构；
2. 当数据结构填满后，如果分区中还有未处理的数据记录，就对结构中的数据记录按（目标分区 ID，Key）排序，将所有数据溢出到临时文件，同时清空数据结构；
3. 重复前 2 个步骤，直到分区中所有的数据记录都被处理为止；
4. 对所有临时文件和内存数据结构中剩余的数据记录做归并排序，生成数据文件和索引文件。

### Shuffle Read

对于所有 Map Task 生成的中间文件，Reduce Task 需要通过网络从不同节点的硬盘中下载并拉取属于自己的数据内容；不同的 Reduce Task 是根据 index 文件中的起始索引来确定哪些数据内容是“属于自己的”。

*Reduce 阶段的数据拉取的过程叫做 Shuffle Read*。它不同于 Reduce Task 。

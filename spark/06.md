# Shuffle 管理

任务调度的首要环节，是 DAGScheduler 以 Shuffle 为边界，把计算图 DAG 切割为多个执行阶段 Stages。

Shuffle 的计算几乎需要消耗所有类型的硬件资源，Shuffle 中的哈希与排序操作会大量消耗 CPU，而 Shuffle Write 生成中间文件的过程，会消耗宝贵的内存资源与磁盘 I/O，最后，Shuffle Read 阶段的数据拉取会引入大量的网络 I/O。

- Shuffle 是衔接不同执行阶段的关键环节；
- Shuffle 是资源密集型计算；
- Shuffle 的执行性能往往是 Spark 作业端到端执行效率的关键环节。

## Shuffle 是什么

Shuffle 的本意是扑克的“洗牌”，在分布式计算场景中，它被引申为**集群范围内跨节点、跨进程的数据分发**。

以工地搬砖任务来理解 Shuffle，把不同类型的砖头看作是分布式数据集，不同类型的砖头在各个分公司之间搬运的过程，这与分布式计算中的 Shuffle 可以说是异曲同工。

<img src="https://static001.geekbang.org/resource/image/4d/80/4d93f366247018896373c1d846fb4780.jpg?wh=1920x1403" width="50%" />

在 Shuffle 的过程，分布式数据集在集群内的分发，会引入大量的磁盘 I/O 与网络 I/O。在 DAG 的计算链条中，Shuffle 环节的执行性能是最差的，那为什么在计算的过程中非要引入 Shuffle 操作呢？

计算过程之所以需要 Shuffle，往往是由计算逻辑或业务逻辑决定的。结合实际的工作经验来说，在绝大多数的业务场景中，Shuffle 操作都是必需的。

比如，对于搬砖任务来说，不同的建筑项目就是需要不同的建材，只有这样才能满足不同的施工要求。  
再比如，在 Word Count 的例子中，“业务逻辑”是对单词做统计计数，那么对单词“Spark”来说，在做“加和”之前，就得把原本分散在不同 Executors 中的“Spark”，拉取到某一个 Executor，才能完成统计计数的操作。

## Shuffle 工作原理

以 Word Count 的 reduceByKey 为例，

[参考代码](./src/c01/word-count.scala)

```scala
val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) => x + y) 
```

<img src="https://static001.geekbang.org/resource/image/31/4d/3199582354a56f9e64bdf7b8a516b04d.jpg?wh=1920x1256" width="50%" />

以 Shuffle 为边界，reduceByKey 的计算被切割为两个执行阶段。把 Shuffle 之前的 Stage 叫作 Map 阶段，而把 Shuffle 之后的 Stage 称作 Reduce 阶段。
- 在 Map 阶段，每个 Executors 先把自己负责的数据分区做初步聚合（又叫 Map 端聚合、局部聚合）；
- 在 Shuffle 环节，不同的单词被分发到不同节点的 Executors 中；
- 在 Reduce 阶段，Executors 以单词为 Key 做第二次聚合（又叫全局聚合），从而完成统计计数的任务。

以上结论：与其说 Shuffle 是跨节点、跨进程的数据分发，不如说 Shuffle 是 Map 阶段与 Reduce 阶段之间的数据交换。

### Shuffle 中间文件

Map 阶段与 Reduce 阶段，通过 Map 阶段 生产 Shuffle 中间文件，Reduce 阶段消费这个中间文件，以中间文件为媒介，来完成集群范围内的数据交换。

**中间文件的产生和消费过程**

<img src="https://static001.geekbang.org/resource/image/95/80/95479766b8acebdedd5c8a0f8bda0680.jpg?wh=1920x862" width="60%" />

解释：

- 调度系统里 DAGScheduler 会为每一个 Stage 创建任务集合 TaskSet，而每一个 TaskSet 都包含多个分布式任务（Task）。
- 在 Map 执行阶段，每个 Task（以下简称 Map Task）都会生成 Shuffle 中间文件。中间文件是以 Map Task 为粒度生成的，Map 阶段有多少个 Map Task，就会生成多少份 Shuffle 中间文件。
- Shuffle 中间文件是统称、泛指，它包含两类实体文件：*记录（Key，Value）键值对的 data 文件* 和 *记录键值对所属 Reduce Task 的 index 文件*。index 文件标记了 data 文件中的哪些记录，应该由下游 Reduce 阶段中的哪些 Task（简称 Reduce Task）消费。
- 在 Spark 中，*Shuffle 环节数据交换规则又叫分区规则*，它定义了分布式数据集在 Reduce 阶段如何划分数据分区。

   假设 Reduce 阶段有 N 个 Task，这 N 个 Task 对应着 N 个数据分区，那么在 Map 阶段，每条记录应该分发到 Reduce Task 的 P 号分区，公式如下，
   ```scala
   P = Hash(Record Key) % N
   ```

>对于每一个 Map Task 生成的中间文件，其中的目标分区数量是由 Reduce 阶段的任务数量（又叫并行度）决定的。  
>在上面的示意图中，Reduce 阶段的并行度是 3，因此，Map Task 的中间文件会包含 3 个目标分区的数据， index 文件是用来标记目标分区所属数据记录的起始索引。  
>对 Map Task ，其并行度由其 Stage 中的首个 RDD 决定，如果 Map Task 是读取 HDFS，那么并行度就是分布式文件 block 数量。

### Shuffle Write

*Shuffle 阶段生成中间文件的过程，又叫 Shuffle Write*。

<img src="https://static001.geekbang.org/resource/image/92/ab/92781f6ff67224812d7aee1b7d6a63ab.jpg?wh=1920x618" width="80%" />

解释：

在生成中间文件的过程中，Spark 会借助一种类似于 Map 的数据结构，来计算、缓存并排序数据分区中的数据记录。这种 Map 结构的 Key 是（Reduce Task Partition ID，Record Key），而 Value 是原数据记录中的数据值，如图中的“内存数据结构”所示。

对于数据分区中的数据记录，Spark 会根据前面提到的公式逐条计算记录所属的目标分区 ID，然后把主键（Reduce Task Partition ID，Record Key）和记录的数据值插入到 Map 数据结构中。当 Map 结构被灌满之后，Spark 根据主键对 Map 中的数据记录做排序，然后把所有内容溢出到磁盘中的临时文件。

随着 Map 结构被清空，Spark 可以继续读取分区内容并继续向 Map 结构中插入数据，直到 Map 结构再次被灌满而再次溢出。就这样，如此往复，直到数据分区中所有的数据记录都被处理完毕。

到此，磁盘上存有若干个溢出的临时文件，而内存的 Map 结构中也留有部分数据，接着 Spark 使用归并排序算法对所有临时文件和 Map 结构剩余数据做合并，分别生成 data 文件和与之对应的 index 文件。

Shuffle Write 过程中生成中间文件的详细过程，可归纳为 4 个步骤：
1. 对于数据分区中的数据记录，逐一计算其目标分区，然后填充内存数据结构；
2. 当数据结构填满后，如果分区中还有未处理的数据记录，就对结构中的数据记录按（目标分区 ID，Key）排序，接着将所有数据溢出到临时文件，同时清空数据结构；
3. 重复前 2 个步骤，直到分区中所有的数据记录都被处理为止；
4. 对所有临时文件和内存数据结构中剩余的数据记录做归并排序，生成数据文件和索引文件。

### Shuffle Read

*Reduce 阶段的数据拉取的过程叫做 Shuffle Read*。

对于所有 Map Task 生成的中间文件，Reduce Task 需要通过网络从不同的节点的硬盘中拉取属于自己的数据内容；不同的 Reduce Task 是根据 index 文件中的起始索引来确定哪些数据内容是“属于自己的”。（Reduce Task 从自己的起始索引，读到下一个 Task 的起始索引，就确定了自己的数据范围）

## 小结

- 以 Shuffle 为边界，把计算图 DAG 切割为 Map 阶段与 Reduce 阶段，以中间文件为媒介完成数据的交换；
- Map 阶段，Shuffle Write 生成中间文件，中间文件的数量由 Reduce 阶段的任务并行度决定； 
- Reduce 阶段，Reduce Task 通过 index 文件来“定位”属于自己的数据内容，并通过网络从不同节点的 data 文件中下载数据（Shuffle Read）。

### 问答

在 Shuffle 的计算过程中，中间文件存储在参数 spark.local.dir 设置的文件目录中，这个参数的默认值是 /tmp，这个参数该如何设置才更合理呢？

查阅 [参考文档](https://spark.apache.org/docs/latest/configuration.html#application-properties)

```
Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks.
Note: This will be overridden by SPARK_LOCAL_DIRS (Standalone), MESOS_SANDBOX (Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.
```

`spark.local.dir` 配置的是 Spark “暂存”目录，用来存 map output 文件（ Shuffle 产生）和保存 RDD 到磁盘。应该配置成快速的本地磁盘。可以用‘,’分隔的多个目录。

首先 /tmp 是系统临时文件目录，会被系统清理的（取决于不同 Linux 分发版的清理策略），如果作业运行时 /tmp 中的文件被清除了，那就要重新shuffle或重新缓存 RDD 。因此，不适合将配置设置为默认的 /tmp 。

其次，可以考虑配置多块硬盘（或者使用 SSD），再把不同硬盘的目录配置到 spark.local.dir，这样可以提升 Shuffle 和 RDD 缓存的性能。

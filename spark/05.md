# 调度系统

把抽象的计算图，转化为实实在在的分布式计算任务，然后以并行计算的方式交付执行。——分布式计算的精髓。

## Spark 调度系统关键步骤与核心组件

| 步骤序号 | 调度步骤 | 所在进程 | 核心组件 |
|--|--|--|--|
| 1 | 将 DAG 拆分成不同的运行阶段 Stages；<br />并根据 Stages 创建分布式任务 Tasks 和任务组 TaskSets | Driver | DAGScheduler |
| 2 | 获取集群中可用计算资源 | Driver | SchudlerBackend |
| 3 | 按照调度规则决定任务优先级，完成任务调度 | Driver | TaskScheduler |
| 4 | 依序将分布式任务分发到 Executors | Driver | SchedulerBackend |
| 5 | 并发执行接收到的分布式计算任务 | Executors | ExecutorBackends |

**TaskScheduler 和 SchedulerBackend 是“元老”**

在 SparkContext / SparkSession 的初始化中，TaskScheduler 和 SchedulerBackend 是最早、且同时被创建的调度系统组件。二者的关系非常微妙：SchedulerBackend 在构造方法中引用 TaskScheduler，而 TaskScheduler 在初始化时会引用 SchedulerBackend。

*SchedulerBackend 与资源管理器（Standalone、YARN、Mesos 等）强绑定，是资源管理器在 Spark 中的代理。*
SchedulerBackend 组件的实例化，取决于开发者指定的 Spark MasterURL，如 `–master spark://ip:host` 就代表 Standalone 部署模式，`–master yarn` 就代表 YARN 模式等等。

**调度系统全局视角**

<img src="https://static001.geekbang.org/resource/image/49/2f/4978695a7560ab08f3e5fd08b4ee612f.jpg?wh=1920x792" width="70%" />

任务调度的 5 个步骤：

1. DAGScheduler 以 Shuffle 为边界，将开发者设计的计算图 DAG 拆分为多个执行阶段 Stages，然后为每个 Stage 创建任务集 TaskSet。
2. SchedulerBackend 通过与 Executors 中的 ExecutorBackend 的交互来实时地获取集群中可用的计算资源，并将这些信息记录到 ExecutorDataMap 数据结构。
3. 与此同时，SchedulerBackend 根据 ExecutorDataMap 中可用资源创建 WorkerOffer，以 WorkerOffer 为粒度提供计算资源。
4. 对于给定 WorkerOffer，TaskScheduler 结合 TaskSet 中任务的本地性倾向，按照 PROCESS_LOCAL、NODE_LOCAL、RACK_LOCAL 和 ANY 的顺序，依次对 TaskSet 中的任务进行遍历，优先调度本地性倾向要求苛刻的 Task。
5. 被选中的 Task 由 TaskScheduler 传递给 SchedulerBackend，再由 SchedulerBackend 分发到 Executors 中的 ExecutorBackend。Executors 接收到 Task 之后，即调用本地线程池来执行分布式任务。

## DAGScheduler

作为集团公司的总架构师，核心职责是把计算图 DAG 拆分为执行阶段 Stages，同时还要负责把 Stages 转化为任务集合 TaskSets（也就是把“建筑图纸”转化成可执行、可操作的“建筑项目”）。

- DAGScheduler 从 DAG 到 Stages 的拆分过程：*以 Actions 算子为起点，从后向前回溯 DAG，以 Shuffle 操作为边界去划分 Stages*。
- DAGScheduler 是任务调度的发起者，DAGScheduler 以 TaskSet 为粒度，向 TaskScheduler 提交任务调度请求。

DAGScheduler 的主要职责有三个：
- 根据用户代码构建 DAG；
- 以 Shuffle 为边界切割 Stages；
- 基于 Stages 创建 TaskSets，并将 TaskSets 提交给 TaskScheduler 请求调度。

以 Word Count 为例， Spark 作业的运行分为两个环节：
- 以惰性的方式构建计算图

<img src="https://static001.geekbang.org/resource/image/24/1c/249eb09407421838782f2515f09yy01c.jpg?wh=1920x534" width="70%" />

- 通过 Actions 算子触发作业的从头计算

Spark从后向前，以递归的方式，依次提请执行所有的 Stages。

具体来说，DAGScheduler 最先提请执行的是 Stage1。在提交的时候，DAGScheduler 发现 Stage1 依赖的父 Stage，也就是 Stage0，还没有执行过，那么这个时候它会把 Stage1 的提交动作压栈，转而去提请执行 Stage0。当 Stage0 执行完毕的时候，DAGScheduler 通过出栈的动作，再次提请执行 Stage 1。

<img src="https://static001.geekbang.org/resource/image/61/d3/61f394b4bc31af6847944911032119d3.jpg?wh=1920x503" width="70%" />

## SchedulerBackend

作为集团公司的人力资源总监，核心职责就是实时汇总并掌握全公司的人力资源状况。这里，全公司的人力资源对应的就是 Spark 的计算资源。对于集群中可用的计算资源，SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构，来记录每一个计算节点中 Executors 的资源状态。

ExecutorDataMap 是一种 HashMap，它的 Key 是标记 Executor 的字符串，Value 是一种叫做 ExecutorData 的数据结构。
ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，它相当于是对 Executor 做的“资源画像”。

ExecutorDataMap 相当于是“人力资源小册子”，对内，SchedulerBackend 可以就 Executor 做“资源画像”；对外，SchedulerBackend 以 WorkerOffer 为粒度提供计算资源。其中，WorkerOffer 封装了 Executor ID、主机地址和 CPU 核数，它用来表示一份可用于调度任务的空闲资源。

SchedulerBackend 与集群内所有 Executors 中的 ExecutorBackend 保持周期性通信，双方通过 LaunchedExecutor、RemoveExecutor、StatusUpdate 等消息来互通有无、变更可用计算资源。

## TaskScheduler

作为施工经理，核心职责是遴选出最合适的“活儿”并派发到“人力”。这个遴选的过程，就是任务调度的核心所在。

- TaskScheduler 在初始化的过程中，会创建任务调度队列，任务调度队列用于缓存 DAGScheduler 提交的 TaskSets。
- TaskScheduler 结合 SchedulerBackend 提供的 WorkerOffer，按照预先设置的调度策略依次对队列中的任务进行调度。

对于 SchedulerBackend 提供的一个个 WorkerOffer，TaskScheduler 是依据什么规则来挑选 Tasks 的呢？
对于给定的 WorkerOffer，TaskScheduler 是按照任务的本地倾向性，来遴选出 TaskSet 中适合调度的 Tasks。

每个任务都是自带本地倾向性的，换句话说，每个任务都有自己的“调度意愿”。

这种定向到计算节点粒度的本地性倾向，Spark 中的术语叫做 NODE_LOCAL。除了定向到节点，Task 还可以定向到进程（Executor）、机架、任意地址，它们对应的术语分别是 PROCESS_LOCAL、RACK_LOCAL 和 ANY。对于倾向 PROCESS_LOCAL 的 Task 来说，它要求对应的数据分区在某个进程（Executor）中存有副本；而对于倾向 RACK_LOCAL 的 Task 来说，它仅要求相应的数据分区存在于同一机架即可。ANY 则等同于无定向，也就是 Task 对于分发的目的地没有倾向性，被调度到哪里都可以。

本地性倾向则意味着代码和数据应该在哪里“相会”，PROCESS_LOCAL 是在 JVM 进程中，NODE_LOCAL 是在节点内，RACK_LOCAL 是不超出物理机架的范围，而 ANY 则代表“无所谓、不重要”。

Spark 调度系统的核心思想，是“数据不动、代码动”。也就是说，在任务调度的过程中，为了完成分布式计算，Spark 倾向于让数据待在原地、保持不动，而把计算任务（代码）调度、分发到数据所在的地方，从而消除数据分发引入的性能隐患。毕竟，相比分发数据，分发代码要轻量得多。

举例，

<img src="https://static001.geekbang.org/resource/image/49/6b/495d8ebf85758b4ba5daa5e562da736b.jpg?wh=1920x695" width="70%" />

从 PROCESS_LOCAL、NODE_LOCAL、到 RACK_LOCAL、再到 ANY，Task 的本地性倾向逐渐从严苛变得宽松。
TaskScheduler 接收到 WorkerOffer 之后，也正是按照这个顺序来遍历 TaskSet 中的 Tasks，优先调度本地性倾向为 PROCESS_LOCAL 的 Task，而 NODE_LOCAL 次之，RACK_LOCAL 为再次，最后是 ANY。

## ExecutorBackend

作为分公司的人力资源主管，核心职责就是接到总公司的“活儿”之后，把活儿派发给分公司的建筑工人。这些工人，就是 Executors 线程池中一个又一个的 CPU 线程，每个线程负责处理一个 Task。

每当 Task 处理完毕，这些线程便会通过 ExecutorBackend，向 Driver 端的 SchedulerBackend 发送 StatusUpdate 事件，告知 Task 执行状态。接下来，TaskScheduler 与 SchedulerBackend 通过接力的方式，最终把状态汇报给 DAGScheduler，如上图中步骤 7、8、9 所示。

如上图中步骤 1 ～ 9 的计算过程，Spark 调度系统就完成了 DAG 中某一个 Stage 的任务调度。不过一个 DAG 会包含多个 Stages，一个 Stage 的结束即宣告下一个 Stage 的开始。只有当所有的 Stages 全部调度、执行完毕，才表示一个完整的 Spark 作业宣告结束。

## Task

Task 代表的是分布式任务，它的关键属性如下：
- name
- stageid ：与 stageAttemptId 标记了 Task 与执行阶段 Stage 的所属关系；
- stageAttempId
- taskBinary ：封装了隶属于这个执行阶段的用户代码
- partition ： RDD 数据分区
- Iocs ：以字符串的形式记录了该任务倾向的计算节点或是 Executor ID。
>Task 与 RDD 的 partitions 是一一对应的，在创建 Task 的过程中，DAGScheduler 会根据数据分区的物理地址，来为 Task 设置 locs 属性。  
>taskBinary、partition 和 locs 这三个属性，一起描述了：Task 应该在哪里（locs）为谁（partition）执行什么任务（taskBinary）。

# RDD 常用算子

## 创建 RDD

在 Spark 中，创建 RDD 的典型方式有两种：

- 通过 SparkContext.parallelize 在内部数据之上创建 RDD；
- 通过 SparkContext.textFile 等 API 从外部数据创建 RDD。

这里的内部、外部是相对应用程序来说的。开发者在 Spark 应用中自定义的各类数据结构，如数组、列表、映射等，都属于“内部数据”；而“外部数据”指代的，是 Spark 系统之外的所有数据形式，如本地文件系统或是分布式文件系统中的数据，再比如来自其他大数据组件（Hive、Hbase、RDBMS 等）的数据。

第一种方式，Parallelize API 的典型用法，是在“小数据”之上创建 RDD。

```scala
import org.apache.spark.rdd.RDD
val words: Array[String] = Array("Spark", "is", "cool")
val rdd: RDD[String] = sc.parallelize(words)
```

要想在真正的“大数据”之上创建 RDD，还得依赖第二种创建方式：

```scala
import org.apache.spark.rdd.RDD
val rootPath: String = _
val file: String = s"${rootPath}/wikiOfSpark.txt"
// 读取文件内容
val lineRDD: RDD[String] = spark.sparkContext.textFile(file)
```

## RDD 内的数据转换 —— 算子

### map：以元素为粒度的数据转换

```scala
// 把RDD元素转换为（Key，Value）的形式

// 定义映射函数f
def f(word: String): (String, Int) = {
    return (word, 1)
}

val cleanWordRDD: RDD[String] = _ // 请参考第一讲获取完整代码
val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(f)
```

map(f) 是以元素为粒度对 RDD 做数据转换的，在某些计算场景下，这个特点会严重影响执行效率。比如，

```scala
val kvRDD: RDD[(String, Int)] = cleanWordRDD.map{ word =>
    // 获取MD5对象实例
    val md5 = MessageDigest.getInstance("MD5")
    // 使用MD5计算哈希值
    val hash = md5.digest(word.getBytes).mkString
    // 返回哈希值与数字1的Pair
    (hash, 1)
}
```

### mapPartitions：以数据分区为粒度的数据转换

以数据分区为粒度，使用映射函数 f 对 RDD 进行数据转换。对于上述单词哈希值计数的例子，来看看如何使用 mapPartitions 来改善执行性能：

```scala
val kvRDD: RDD[(String, Int)] = cleanWordRDD.mapPartitions( partition => {
    // 注意！这里是以数据分区为粒度，获取MD5对象实例
    val md5 = MessageDigest.getInstance("MD5")
    val newPartition = partition.map( word => {
    // 在处理每一条数据记录的时候，可以复用同一个Partition内的MD5对象
    (md5.digest(word.getBytes()).mkString,1)
    })
    newPartition
})
```

以数据分区为单位，mapPartitions 只需实例化一次 MD5 对象，而 map 算子却需要实例化多次，具体的次数则由分区内数据记录的数量来决定。

![](https://static001.geekbang.org/resource/image/c7/8d/c76be8ff89f1c37e52e9f17b66bf398d.jpg?wh=1920x779)

相比 mapPartitions，mapPartitionsWithIndex 仅仅多出了一个数据分区索引，这个索引可以作为分区编号，当业务逻辑中需要使用到分区编号的时候，不妨考虑使用这个算子来实现代码。

### flatMap：从元素到集合、再从集合到元素

flatMap 也是用来做数据映射的，在实现上，对于给定映射函数 f，flatMap(f) 以元素为粒度，对 RDD 进行数据转换。

与 map 和 mapPartition 相比，flatMap 的映射函数 f 有着显著的不同。对于 map 和 mapPartitions 来说，其映射函数 f 的类型，都是`（元素） => （元素）`，即元素到元素。而 flatMap 映射函数 f 的类型，是`（元素） => （集合）`，即元素到集合（如数组、列表等）。因此，flatMap 的映射过程在逻辑上分为两步：

- 以元素为单位，创建集合；
- 去掉集合“外包装”，提取集合元素。

```scala
val wordPairRDD: RDD[String] = lineRDD.flatMap( line => {
    // 将行转换为单词数组
    val words: Array[String] = line.split(" ")
    // 将单个单词数组，转换为相邻单词数组
    for (i <- 0 until words.length - 1) yield words(i) + "-" + words(i+1)
})
```

![](https://static001.geekbang.org/resource/image/a6/bd/a6bcd12fbc377405557c1aaf63cd24bd.jpg?wh=1920x840)

### filter：过滤 RDD

filter 算子需要借助一个判定函数 f，才能实现对 RDD 的过滤转换。所谓判定函数，它指的是类型为（RDD 元素类型） => （Boolean）的函数。

以过滤掉像“Spark-&”、“|-data”这样的词对为例：

```scala
// 定义特殊字符列表
val list: List[String] = List("&", "|", "#", "^", "@")
 
// 定义判定函数f
def f(s: String): Boolean = {
    val words: Array[String] = s.split("-")
    val b1: Boolean = list.contains(words(0))
    val b2: Boolean = list.contains(words(1))
    return !b1 && !b2 // 返回不在特殊字符列表中的词汇对
}
 
// 使用filter(f)对RDD进行过滤
val cleanedPairRDD: RDD[String] = wordPairRDD.filter(f)
```

## 参考

- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)

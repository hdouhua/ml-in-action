# Hello Spark

## install spark

prerequisites: Java, scala.

- install java

```shell
sudo apt-get update
sudo apt install default-jdk

java -version

# set JAVA_HOME
update-alternatives --config java
# /usr/lib/jvm/java-11-openjdk-amd64
sudo vi /etc/environment
source /etc/environment
echo $JAVA_HOME
```

- install scala

```shell
sudo apt-get install scala

scala
>println("Hello World!")
>:q
```

- install spark

get package download URL from https://spark.apache.org/downloads.html

```shell
wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
tar -zxf spark-3.3.1-bin-hadoop3.tgz

# add spark-shell bin folder to PATH
sudo vi /etc/environment
source /etc/environment
spark-shell --version
```

## install pyspark via conda

in this way, try to run spark in Jupyter.

```shell
# update conda
conda update -n base -c defaults conda

# create env
conda create -n spark
conda activate spark

# install pyspark
conda install -c conda-forge pyspark

conda install -n spark ipykernel --update-deps --force-reinstall
```

## example: word count

主要包含如下 3 个步骤：

- 读取内容：调用 Spark 文件读取 API，加载 wikiOfSpark.txt 文件内容；
- 分词：以行为单位，把句子打散为单词；
- 分组计数：按照单词做分组计数。

Spark 支持种类丰富的开发语言，如 Scala、Java、Python，等等。结合个人偏好和开发习惯，任意选择其中的一种进行开发。

参考 [scala 代码](./src/c01/word-count.scala) 或者 [ipython notebook](./src/c01/word-count.ipynb)

```shell
spark-shell

scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> val rootPath: String = "/home/dao/spk"
rootPath: String = /home/dao/spk

scala> val file: String = s"${rootPath}/wikiOfSpark.txt"
file: String = /home/dao/spk/wikiOfSpark.txt

scala> val lineRDD: RDD[String] = spark.sparkContext.textFile(file)
lineRDD: org.apache.spark.rdd.RDD[String] = /home/dao/spk/wikiOfSpark.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val wordRDD: RDD[String] = lineRDD.flatMap(line => line.split(" "))
wordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:24

scala> val cleanWordRDD: RDD[String] = wordRDD.filter(word => !word.equals(""))
cleanWordRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:24

scala> val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word => (word, 1))
kvRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[4] at map at <console>:24

scala> val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) => x + y)
wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:24

scala> wordCounts.map{case (k, v) => (v, k)}.sortByKey(false).take(10)
res1: Array[(Int, String)] = Array((67,the), (63,Spark), (54,a), (51,and), (50,of), (35,in), (34,Apache), (33,to), (25,is), (23,for))
```

**代码解释：**

spark 和 sparkContext 分别是两种不同的开发入口实例：
- spark 是开发入口 SparkSession 实例（Instance），SparkSession 在 spark-shell 中会由系统自动创建；
- sparkContext 是开发入口 SparkContext 实例。

>从 2.0 版本开始，SparkSession 取代了 SparkContext，成为统一的开发入口。

RDD 的全称是 Resilient Distributed Dataset，意思是“弹性分布式数据集”。RDD 是 Spark 对于分布式数据的统一抽象，它定义了一系列分布式数据的基本属性与处理方法。在此，先简单地把 RDD 理解成“数组”。

flatMap 操作在逻辑上可以分成两个步骤：映射和展平。

在 RDD 的开发框架下，聚合类操作，如计数、求和、求均值，需要依赖键值对（Key/Value Pair）类型的数据元素，也就是（Key，Value）形式的“数组”元素。
因此，在调用聚合算子做分组计数之前，先把 RDD 元素转换为（Key，Value）的形式，也就是 (Key, 1)。

对于 （Key，Value） 这个键值对“数组”，reduceByKey 先是按照 Key（也就是单词）来做分组，分组之后，每个单词都有一个与之对应的 Value 列表。然后根据用户提供的聚合函数，对同一个 Key 的所有 Value 做 reduce 运算。

## hadoop vs spark

往小了说，Hadoop 特指 `HDFS`、`YARN`、`MapReduce` 这三个组件，他们分别是 Hadoop 分布式文件系统、分布式任务调度框架、分布式计算引擎。

往大了说，Hadoop 生态包含所有由这3个组件衍生出的大数据产品，如 Hive、Hbase、Pig、Sqoop，等等。

Spark 和 Hadoop 的关系，是共生共赢的关系。Spark 的定位是分布式计算引擎，因此，它的直接“竞争对手”，是 MapReduce，也就是 Hadoop 的分布式计算引擎。Spark 是内存计算引擎，而 MapReduce 在计算的过程中，需要频繁落盘，因此，一般来说，相比 MapReduce，Spark 在执行性能上，更胜一筹。

对于 HDFS、YARN，Spark 可与之完美结合，实际上，在很多的使用场景中，Spark 的数据源往往存储于 HDFS，而YARN 是 Spark 重要的资源调度框架之一。

## 参考

- [PySpark Installation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html#python-version-supported)
- [Quick Start - Spark](https://spark.apache.org/docs/latest/quick-start.html)
- [Spark Core API reference for Python](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis)

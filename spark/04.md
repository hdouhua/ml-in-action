# Spark 的分布式计算

计算流图是一种抽象的流程图，计算流图中的每一个元素，如 Word Count 计算流图中的 lineRDD、wordRDD，都是“虚”的数据集抽象。

分布式计算的精髓，在于如何把抽象的计算流图（“设计图纸”），转化为实实在在的分布式计算任务（“流水线工艺”），然后以并行计算的方式交付执行。

Spark 分布式计算的实现，离不开两个关键要素：进程模型 和 分布式的环境部署。

## 进程模型

<u>在 Spark 的应用开发中</u>，任何一个应用程序的入口，都是带有 SparkSession 的 main 函数。SparkSession 包罗万象，它在提供 Spark 运行时上下文（如调度系统、存储系统、内存管理、RPC 通信）的同时，也为开发者提供创建、转换、计算分布式数据集（如 RDD）的 API。

<u>在 Spark 分布式计算环境中</u>，有且仅有一个 JVM 进程运行这样的 main 函数，这个特殊的 JVM 进程，在 Spark 中有个专门的术语，叫作“Driver”。Driver 最核心的作用在于：解析用户代码、构建计算流图，然后将计算流图转化为分布式任务，并把任务分发给集群中的执行进程 (Executor) 交付运行。通俗弟说，Driver 的角色是拆解任务、派活儿，而真正干活儿的“苦力”是执行进程。在 Spark 的分布式环境中，Executor 可以有一个或是多个。

<img src="https://static001.geekbang.org/resource/image/de/36/de80376be9c39600ab7c4cc109c8f336.jpg?wh=1920x1503" width="50%" /><br />
(Driver 和 Executor 的关系)

分布式计算的核心是任务调度，而分布式任务的调度与执行，仰赖的是 Driver 与 Executors 之间的通力合作。

在 Spark 的 Driver 进程中，DAGScheduler、TaskScheduler 和 SchedulerBackend 这三个对象通力合作，依次完成分布式任务调度的 3 个核心步骤：
1. 根据用户代码构建计算流图；
2. 根据计算流图拆解出分布式任务；
3. 将分布式任务分发到 Executors 中去。

接收到任务之后，Executors 调用内部线程池，结合事先分配好的数据分片，并发地执行任务代码。对于一个完整的 RDD，每个 Executors 负责处理这个 RDD 的一个数据分片子集。

以 Word Count 为例，拆解 Driver 和 Executor 的合作

<img src="https://static001.geekbang.org/resource/image/b0/22/b05139c82a7882a5b3b3074f3be50d22.jpg?wh=1920x952" width="80%" />

Driver 通过 take 这个 Action 算子，来触发执行先前构建好的计算流图。沿着计算流图的执行方向，也就是图中从上到下的方向，Driver 以 Shuffle 为边界创建、分发分布式任务。也就是在 reduceByKey 之前的所有操作为边界， textFile、flatMap、filter、map 等，Driver 会把它们“捏合”成一份任务，然后一次性地把这份任务打包、分发给每一个 Executors。

三个 Executors 接收到任务之后，先是对任务进行解析，把任务拆解成 textFile、flatMap、filter、map 这 4 个步骤，然后分别对自己负责的数据分片进行处理。

在不同 Executors 完成单词的数据交换之后，Driver 继续创建并分发下一个阶段的任务，也就是按照单词做分组计数。数据交换之后，所有相同的单词都分发到了相同的 Executors 上，各个 Executors 拿到 reduceByKey 的任务，只需要各自独立地去完成统计计数即可。

完成计数之后，Executors 会把最终的计算结果统一返回给 Driver。

这样，spark-shell 便完成了 Word Count 用户代码的计算过程。

**补充说明 shuffle**

Shuffle 的本意是扑克牌中的“洗牌”，在大数据领域的引申义，表示的是集群范围内跨进程、跨节点的数据交换。继续以 Word Count 的例子，来简单地对 Shuffle 进行理解。同一个单词，比如“spark”，可能散落在不用的 Executors 进程，要完成对“spark”的计数，需要把所有“spark”分发到同一个 Executor 进程。而这个把原本散落在不同 Executors 的单词，分发到同一个 Executor 的过程，就是 Shuffle。

## spark-shell

spark-shell 有很多命令行参数，其中最为重要的有两类：一类是用于指定部署模式的 master，另一类则用于指定集群的计算资源容量。

```shell
spark-shell
# ==
spark-shell --master local[*]
```

这行代码的含义有两层。
- 第一层含义是部署模式，其中 local 关键字表示部署模式为 Local，也就是本地部署；
- 第二层含义是部署规模，也就是方括号里面的数字，它表示的是在本地部署中需要启动多少个 Executors，星号则意味着这个数量与机器中可用 CPU 的个数相一致。

## 分布式环境部署

Spark 真正的威力，在于分布式集群中的并行计算。也只有充分利用集群中每个节点的计算资源，才能充分发挥出 Spark 的性能优势。

Spark 支持多种分布式部署模式，如 Standalone、YARN、Mesos、Kubernetes。其中 Standalone 是 Spark 内置的资源调度器，而 YARN、Mesos、Kubernetes 是独立的第三方资源调度与服务编排框架。

为了示意，下面以 Standalone 模式的部署为例。

Standalone 在资源调度层面，采用了一主多从的主从架构，把计算节点的角色分为 Master 和 Worker。其中，Master 有且只有一个，而 Worker 可以有一到多个。所有 Worker 节点周期性地向 Master 汇报本节点可用资源状态，Master 负责汇总、变更、管理集群中的可用资源，并对 Spark 应用程序中 Driver 的资源请求作出响应。

1. 所有节点上准备 Java 环境并安装 Spark，[参考](./01.md#install-spark)
   - 设置 Master 主机名 vm1
   - 设置 Worker 主机名 vm2
   - 设置 Worker 主机名 macos

   - 配置 Master config （可选）
   
      ```shell
      cp conf/spark-defaults.conf.template conf/spark-defaults.conf

      vi conf/spark-defaults.conf
      spark.master     spark://vm1:7077
      ```
      >有些配置参数可以通过 环境变量 设置，相关配置文件 `conf/spark-env.sh`（第三方资源调度器不会读取） 或 `conf/spark-defaults.conf`

2. 依次启动 Master 和 Worker 节点

   ```shell
   $SPARK_HOME/sbin/stop-master.sh
   $SPARK_HOME/sbin/start-master.sh
   # wihtout setting conf/spark-defaults.conf
   MASTER=spark://vm1:7077 $SPARK_HOME/sbin/start-master.sh

   $SPARK_HOME/sbin/stop-worker.sh
   $SPARK_HOME/sbin/start-worker.sh spark://vm1:7077
   ```

3. 集群启动之后，可以使用 Spark 自带的小例子，来验证 Standalone 分布式部署是否成功。

   ```shell
   # run-example
   MASTER=spark://vm1:7077 $SPARK_HOME/bin/run-example org.apache.spark.examples.SparkPi
   # or spark-submit
   $SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.JavaSparkPi \
      --master spark://vm1:7077 \
      $SPARK_HOME/examples/jars/spark-examples_2.12-3.3.1.jar
   $SPARK_HOME/bin/spark-submit --master spark://vm1:7077 $SPARK_HOME/examples/src/main/python/pi.py 10
   # or by spark-shell
   $SPARK_HOME/bin/pyspark --master spark://vm1:7077
   $SPARK_HOME/bin/spark-shell --master spark://vm1:7077
   ```

**端口** [参考文档](https://spark.apache.org/docs/latest/security.html#standalone-mode-only)

- master node run on the port 7077 (by default)
- master UI http://vm1:8080
- worker UI http://192.168.1.94:8081/
- application UI http://192.168.1.94:4040/

## Standalone 部署模式的一些补充

先说说选主，Standalone 部署模式下，Master 与 Worker 角色，这个通常是通过配置文件，事先配置好的，哪台是 Master，哪台是 Worker。启动时，先启动 Master，然后启动 Worker，由于配置中有 Master 的连接地址，所以 Worker 启动的时候，会自动去连接 Master，然后双方建立心跳机制，随后集群进入 ready 状态。

接下来说 Master、Worker 与 Driver、Executors 的关系。首先，这些都是 JVM 进程。不过它们的定位和角色，是完全不一样的。Master、Worker 用来做资源的调度与分配，简单理解为它们负责维护集群中可用硬件资源的状态。Worker 记录着每个计算节点可用 CPU cores、可用内存等。而 Master 从 Worker 收集并汇总所有集群中节点的可用计算资源。

Driver 和 Executors 的角色，就纯是 Spark 应用级别的进程。Driver、Executors 的计算资源，全部来自于 Master 的调度。一般来说，Driver 会占用 Master 所在节点的资源；而 Executors 占用 Worker 所在节点的计算资源。一旦 Driver、Executors 从 Master、Worker 那里申请到资源之后，Driver、Executors 就不再理会 Master 和 Worker 了，因为资源已经到手了，后续就是任务调度的范畴。

## 参考

- [Standalone Mode](https://spark.apache.org/docs/latest/configuration.html#standalone-mode)
